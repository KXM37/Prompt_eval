{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2774653",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptImprover:\n",
    "    \"\"\"\n",
    "    Generates improved prompts based on evaluation results.\n",
    "    Focuses on boosting below-threshold metrics.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, metric_config: dict, config: dict, api_key: str = None):\n",
    "        self.metric_config = metric_config\n",
    "        self.config = config\n",
    "        self.client = anthropic.Anthropic(api_key=api_key) if api_key else None\n",
    "    \n",
    "    def improve_prompt(self, original_prompt: str, evaluation_result: Dict) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Generate an improved version of the prompt.\n",
    "        \n",
    "        Args:\n",
    "            original_prompt: The original prompt text\n",
    "            evaluation_result: Full evaluation results from PromptEvaluator\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with 'improved_prompt' and 'change_explanations'\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"GENERATING IMPROVED PROMPT\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        below_threshold = evaluation_result['below_threshold_metrics']\n",
    "        \n",
    "        if not below_threshold:\n",
    "            print(\"‚úÖ All metrics above threshold - no improvements needed!\")\n",
    "            return {\n",
    "                'improved_prompt': original_prompt,\n",
    "                'change_explanations': \"All metrics are above threshold. No changes made.\"\n",
    "            }\n",
    "        \n",
    "        print(f\"Targeting {len(below_threshold)} below-threshold metrics...\")\n",
    "        \n",
    "        # Build improvement prompt\n",
    "        improvement_prompt = self._build_improvement_prompt(\n",
    "            original_prompt, \n",
    "            evaluation_result\n",
    "        )\n",
    "        \n",
    "        # Call Claude API\n",
    "        try:\n",
    "            response = self.client.messages.create(\n",
    "                model=\"claude-sonnet-4-20250514\",\n",
    "                max_tokens=8000,\n",
    "                temperature=0.3,  # Slightly creative but controlled\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": improvement_prompt}\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            response_text = response.content[0].text\n",
    "            \n",
    "            # Parse response\n",
    "            result = self._parse_improvement_response(response_text)\n",
    "            \n",
    "            print(\"‚úÖ Improved prompt generated!\")\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error calling Claude API: {e}\")\n",
    "            return {\n",
    "                'improved_prompt': original_prompt,\n",
    "                'change_explanations': f\"Error during improvement: {str(e)}\"\n",
    "            }\n",
    "    \n",
    "    def _build_improvement_prompt(self, original_prompt: str, \n",
    "                                  evaluation_result: Dict) -> str:\n",
    "        \"\"\"Build the prompt for Claude to improve the original prompt.\"\"\"\n",
    "        \n",
    "        below_threshold = evaluation_result['below_threshold_metrics']\n",
    "        metrics_details = evaluation_result['metrics']\n",
    "        \n",
    "        # Group below-threshold metrics by category\n",
    "        by_group = {}\n",
    "        for metric_name in below_threshold:\n",
    "            metric_data = metrics_details[metric_name]\n",
    "            group = self.metric_config[metric_name]['group']\n",
    "            \n",
    "            if group not in by_group:\n",
    "                by_group[group] = []\n",
    "            \n",
    "            gap = metric_data['threshold'] - metric_data['score']\n",
    "            by_group[group].append({\n",
    "                'name': metric_name,\n",
    "                'score': metric_data['score'],\n",
    "                'max': metric_data['max_points'],\n",
    "                'threshold': metric_data['threshold'],\n",
    "                'gap': gap,\n",
    "                'description': self.metric_config[metric_name]['description'],\n",
    "                'evidence': metric_data.get('justification') or metric_data.get('evidence')\n",
    "            })\n",
    "        \n",
    "        # Build detailed metric information\n",
    "        metrics_info_parts = []\n",
    "        for group, metrics in by_group.items():\n",
    "            metrics_info_parts.append(f\"\\n**{group.upper().replace('_', ' ')}:**\")\n",
    "            for m in metrics:\n",
    "                metrics_info_parts.append(\n",
    "                    f\"\\n‚Ä¢ **{m['name']}** - Score: {m['score']:.2f}/{m['max']:.2f} \"\n",
    "                    f\"(Need: {m['threshold']:.2f}, Gap: {m['gap']:.2f})\"\n",
    "                )\n",
    "                metrics_info_parts.append(f\"  - Description: {m['description']}\")\n",
    "                metrics_info_parts.append(f\"  - Current assessment: {m['evidence']}\")\n",
    "        \n",
    "        metrics_info = \"\\n\".join(metrics_info_parts)\n",
    "        \n",
    "        # Create improvement prompt\n",
    "        improvement_prompt = f\"\"\"You are an expert prompt engineer. Your task is to improve a prompt by addressing specific weaknesses identified in an evaluation.\n",
    "\n",
    "ORIGINAL PROMPT:\n",
    "```\n",
    "{original_prompt}\n",
    "```\n",
    "\n",
    "EVALUATION SUMMARY:\n",
    "- Total Score: {evaluation_result['total_score']:.2f} / {evaluation_result['total_max']} ({evaluation_result['percentage']:.1f}%)\n",
    "- Metrics Below Threshold: {len(below_threshold)}\n",
    "- Target: Bring all metrics above their 70% threshold\n",
    "\n",
    "METRICS REQUIRING IMPROVEMENT:\n",
    "{metrics_info}\n",
    "\n",
    "YOUR TASK:\n",
    "Rewrite the prompt to address ALL the metrics listed above that are below threshold. For each metric, implement specific improvements to bring it above the threshold.\n",
    "\n",
    "IMPROVEMENT STRATEGIES:\n",
    "\n",
    "1. **Structure & Clarity**: Add numbered steps, clear sections, explicit task definitions\n",
    "2. **Context & Information**: Provide background, specify output format, include examples\n",
    "3. **Reasoning & Cognition**: Request step-by-step thinking, enable iteration, progressive complexity\n",
    "4. **Safety & Alignment**: Address uncertainty handling, minimize hallucinations, define safe failures\n",
    "5. **Format & Style**: Assign roles/personas, specify audience, calibrate tone\n",
    "6. **Output Quality**: Ensure feasibility, add validation hooks, enable self-correction\n",
    "7. **Advanced Features**: Add memory anchoring, calibration requests, comparisons\n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "- Preserve the core intent and purpose of the original prompt\n",
    "- Add substance, not just keywords (make genuine improvements)\n",
    "- Be specific and actionable in your additions\n",
    "- Don't make the prompt unnecessarily verbose - be concise but complete\n",
    "- Ensure all improvements are relevant to the original task\n",
    "\n",
    "OUTPUT FORMAT:\n",
    "Provide your response in two sections:\n",
    "```\n",
    "IMPROVED PROMPT:\n",
    "[Write the complete improved prompt here]\n",
    "\n",
    "---\n",
    "\n",
    "CHANGES MADE:\n",
    "[For each metric you addressed, explain what specific changes you made and how they improve that metric. Format as a numbered or bulleted list with the metric name and explanation.]\n",
    "```\n",
    "\n",
    "Generate the improved prompt now.\"\"\"\n",
    "        \n",
    "        return improvement_prompt\n",
    "    \n",
    "    def _parse_improvement_response(self, response_text: str) -> Dict[str, str]:\n",
    "        \"\"\"Parse Claude's improvement response.\"\"\"\n",
    "        \n",
    "        # Look for the two sections\n",
    "        try:\n",
    "            # Split on the separator\n",
    "            if \"---\" in response_text:\n",
    "                parts = response_text.split(\"---\", 1)\n",
    "            elif \"CHANGES MADE:\" in response_text:\n",
    "                parts = response_text.split(\"CHANGES MADE:\", 1)\n",
    "                parts[1] = \"CHANGES MADE:\" + parts[1]\n",
    "            else:\n",
    "                # Try to find both sections\n",
    "                improved_start = response_text.find(\"IMPROVED PROMPT:\")\n",
    "                changes_start = response_text.find(\"CHANGES MADE:\")\n",
    "                \n",
    "                if improved_start != -1 and changes_start != -1:\n",
    "                    improved_section = response_text[improved_start:changes_start]\n",
    "                    changes_section = response_text[changes_start:]\n",
    "                    parts = [improved_section, changes_section]\n",
    "                else:\n",
    "                    # Fallback: treat entire response as improved prompt\n",
    "                    return {\n",
    "                        'improved_prompt': response_text.strip(),\n",
    "                        'change_explanations': \"Changes section not found in response.\"\n",
    "                    }\n",
    "            \n",
    "            # Extract improved prompt (remove header)\n",
    "            improved_prompt = parts[0].replace(\"IMPROVED PROMPT:\", \"\").strip()\n",
    "            improved_prompt = improved_prompt.strip('`').strip()\n",
    "            \n",
    "            # Extract change explanations (remove header)\n",
    "            change_explanations = parts[1].replace(\"CHANGES MADE:\", \"\").strip()\n",
    "            change_explanations = change_explanations.strip('`').strip()\n",
    "            \n",
    "            return {\n",
    "                'improved_prompt': improved_prompt,\n",
    "                'change_explanations': change_explanations\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error parsing improvement response: {e}\")\n",
    "            return {\n",
    "                'improved_prompt': response_text.strip(),\n",
    "                'change_explanations': \"Could not parse change explanations.\"\n",
    "            }\n",
    "\n",
    "\n",
    "print(\"‚úÖ Prompt Improver Created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a6e5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_prompt_improver(api_key: str = None, use_mock: bool = True):\n",
    "    \"\"\"Test the prompt improver.\"\"\"\n",
    "    \n",
    "    # Use the result from previous evaluation\n",
    "    if 'result' not in globals():\n",
    "        print(\"‚ö†Ô∏è  Need to run evaluation first!\")\n",
    "        print(\"Run: result, report = test_full_evaluator(use_mock=True)\")\n",
    "        return None\n",
    "    \n",
    "    test_prompt = result['prompt']\n",
    "    \n",
    "    print(f\"Testing Prompt Improver\")\n",
    "    print(f\"Using {'MOCK' if use_mock else 'REAL'} improvement\")\n",
    "    print(f\"\\nOriginal Score: {result['total_score']:.2f}/150 ({result['percentage']:.1f}%)\")\n",
    "    print(f\"Below Threshold: {result['below_threshold_count']} metrics\")\n",
    "    print()\n",
    "    \n",
    "    if use_mock:\n",
    "        # Create mock improvement\n",
    "        improvement_result = _mock_improve_prompt(test_prompt, result)\n",
    "    else:\n",
    "        if not api_key:\n",
    "            print(\"‚ö†Ô∏è  API key required for real improvement\")\n",
    "            return None\n",
    "        improver = PromptImprover(METRIC_CONFIG, CONFIG, api_key=api_key)\n",
    "        improvement_result = improver.improve_prompt(test_prompt, result)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"IMPROVED PROMPT\")\n",
    "    print(\"=\"*80)\n",
    "    print(improvement_result['improved_prompt'])\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"CHANGES MADE\")\n",
    "    print(\"=\"*80)\n",
    "    print(improvement_result['change_explanations'])\n",
    "    \n",
    "    return improvement_result\n",
    "\n",
    "\n",
    "def _mock_improve_prompt(original_prompt: str, evaluation_result: Dict) -> Dict[str, str]:\n",
    "    \"\"\"Mock improvement for testing without API key.\"\"\"\n",
    "    \n",
    "    below_threshold = evaluation_result['below_threshold_metrics']\n",
    "    \n",
    "    # Create mock improved prompt\n",
    "    improvements = []\n",
    "    \n",
    "    # Check what's missing and add it\n",
    "    prompt_lower = original_prompt.lower()\n",
    "    \n",
    "    if any('structure' in m.lower() for m in below_threshold):\n",
    "        improvements.append(\"structured format with numbered steps\")\n",
    "    if any('example' in m.lower() for m in below_threshold):\n",
    "        improvements.append(\"concrete examples\")\n",
    "    if any('role' in m.lower() or 'persona' in m.lower() for m in below_threshold):\n",
    "        improvements.append(\"role assignment\")\n",
    "    if any('reasoning' in m.lower() for m in below_threshold):\n",
    "        improvements.append(\"step-by-step reasoning request\")\n",
    "    if any('validation' in m.lower() for m in below_threshold):\n",
    "        improvements.append(\"validation requirements\")\n",
    "    if any('audience' in m.lower() for m in below_threshold):\n",
    "        improvements.append(\"audience specification\")\n",
    "    \n",
    "    improved_prompt = f\"\"\"# Enhanced Task\n",
    "\n",
    "You are an expert in the domain with deep knowledge.\n",
    "\n",
    "## Original Request\n",
    "{original_prompt}\n",
    "\n",
    "## Additional Requirements\n",
    "1. Think through this step-by-step\n",
    "2. Provide specific examples\n",
    "3. Validate your approach\n",
    "4. Target audience: professionals with intermediate knowledge\n",
    "\n",
    "## Process\n",
    "- Break down the task into clear stages\n",
    "- Verify each stage before proceeding\n",
    "- Iterate on your output for quality\n",
    "\n",
    "Please ensure all criteria are met before providing your final response.\"\"\"\n",
    "    \n",
    "    change_explanations = f\"\"\"**MOCK IMPROVEMENTS APPLIED:**\n",
    "\n",
    "The following enhancements were made to address {len(below_threshold)} below-threshold metrics:\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    for i, improvement in enumerate(improvements, 1):\n",
    "        change_explanations += f\"{i}. Added {improvement}\\n\"\n",
    "    \n",
    "    change_explanations += \"\\n*Note: This is a mock improvement for testing. Use real API for production.*\"\n",
    "    \n",
    "    return {\n",
    "        'improved_prompt': improved_prompt,\n",
    "        'change_explanations': change_explanations\n",
    "    }\n",
    "\n",
    "\n",
    "# Run test\n",
    "print(\"=\"*80)\n",
    "print(\"RUNNING PROMPT IMPROVER TEST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if 'result' in globals():\n",
    "    improvement_result = test_prompt_improver(use_mock=True)\n",
    "    if improvement_result:\n",
    "        print(\"\\n‚úÖ Improvement complete!\")\n",
    "        print(\"   Result stored in 'improvement_result' variable\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Please run evaluation first:\")\n",
    "    print(\"   result, report = test_full_evaluator(use_mock=True)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bbfbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_prompt_improver(api_key: str = None, use_mock: bool = True):\n",
    "    \"\"\"Test the prompt improver.\"\"\"\n",
    "    \n",
    "    # Use the result from previous evaluation\n",
    "    if 'result' not in globals():\n",
    "        print(\"‚ö†Ô∏è  Need to run evaluation first!\")\n",
    "        print(\"Run: result, report = test_full_evaluator(use_mock=True)\")\n",
    "        return None\n",
    "    \n",
    "    test_prompt = result['prompt']\n",
    "    \n",
    "    print(f\"Testing Prompt Improver\")\n",
    "    print(f\"Using {'MOCK' if use_mock else 'REAL'} improvement\")\n",
    "    print(f\"\\nOriginal Score: {result['total_score']:.2f}/150 ({result['percentage']:.1f}%)\")\n",
    "    print(f\"Below Threshold: {result['below_threshold_count']} metrics\")\n",
    "    print()\n",
    "    \n",
    "    if use_mock:\n",
    "        # Create mock improvement\n",
    "        improvement_result = _mock_improve_prompt(test_prompt, result)\n",
    "    else:\n",
    "        if not api_key:\n",
    "            print(\"‚ö†Ô∏è  API key required for real improvement\")\n",
    "            return None\n",
    "        improver = PromptImprover(METRIC_CONFIG, CONFIG, api_key=api_key)\n",
    "        improvement_result = improver.improve_prompt(test_prompt, result)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"IMPROVED PROMPT\")\n",
    "    print(\"=\"*80)\n",
    "    print(improvement_result['improved_prompt'])\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"CHANGES MADE\")\n",
    "    print(\"=\"*80)\n",
    "    print(improvement_result['change_explanations'])\n",
    "    \n",
    "    return improvement_result\n",
    "\n",
    "\n",
    "def _mock_improve_prompt(original_prompt: str, evaluation_result: Dict) -> Dict[str, str]:\n",
    "    \"\"\"Mock improvement for testing without API key.\"\"\"\n",
    "    \n",
    "    below_threshold = evaluation_result['below_threshold_metrics']\n",
    "    \n",
    "    # Create mock improved prompt\n",
    "    improvements = []\n",
    "    \n",
    "    # Check what's missing and add it\n",
    "    prompt_lower = original_prompt.lower()\n",
    "    \n",
    "    if any('structure' in m.lower() for m in below_threshold):\n",
    "        improvements.append(\"structured format with numbered steps\")\n",
    "    if any('example' in m.lower() for m in below_threshold):\n",
    "        improvements.append(\"concrete examples\")\n",
    "    if any('role' in m.lower() or 'persona' in m.lower() for m in below_threshold):\n",
    "        improvements.append(\"role assignment\")\n",
    "    if any('reasoning' in m.lower() for m in below_threshold):\n",
    "        improvements.append(\"step-by-step reasoning request\")\n",
    "    if any('validation' in m.lower() for m in below_threshold):\n",
    "        improvements.append(\"validation requirements\")\n",
    "    if any('audience' in m.lower() for m in below_threshold):\n",
    "        improvements.append(\"audience specification\")\n",
    "    \n",
    "    improved_prompt = f\"\"\"# Enhanced Task\n",
    "\n",
    "You are an expert in the domain with deep knowledge.\n",
    "\n",
    "## Original Request\n",
    "{original_prompt}\n",
    "\n",
    "## Additional Requirements\n",
    "1. Think through this step-by-step\n",
    "2. Provide specific examples\n",
    "3. Validate your approach\n",
    "4. Target audience: professionals with intermediate knowledge\n",
    "\n",
    "## Process\n",
    "- Break down the task into clear stages\n",
    "- Verify each stage before proceeding\n",
    "- Iterate on your output for quality\n",
    "\n",
    "Please ensure all criteria are met before providing your final response.\"\"\"\n",
    "    \n",
    "    change_explanations = f\"\"\"**MOCK IMPROVEMENTS APPLIED:**\n",
    "\n",
    "The following enhancements were made to address {len(below_threshold)} below-threshold metrics:\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    for i, improvement in enumerate(improvements, 1):\n",
    "        change_explanations += f\"{i}. Added {improvement}\\n\"\n",
    "    \n",
    "    change_explanations += \"\\n*Note: This is a mock improvement for testing. Use real API for production.*\"\n",
    "    \n",
    "    return {\n",
    "        'improved_prompt': improved_prompt,\n",
    "        'change_explanations': change_explanations\n",
    "    }\n",
    "\n",
    "\n",
    "# Run test\n",
    "print(\"=\"*80)\n",
    "print(\"RUNNING PROMPT IMPROVER TEST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if 'result' in globals():\n",
    "    improvement_result = test_prompt_improver(use_mock=True)\n",
    "    if improvement_result:\n",
    "        print(\"\\n‚úÖ Improvement complete!\")\n",
    "        print(\"   Result stored in 'improvement_result' variable\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Please run evaluation first:\")\n",
    "    print(\"   result, report = test_full_evaluator(use_mock=True)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40da7230",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_prompts(original: str, improved: str, changes: str):\n",
    "    \"\"\"Create a side-by-side comparison of prompts.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"PROMPT COMPARISON\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    # Stats comparison\n",
    "    print(\"\\nüìä Statistics:\")\n",
    "    print(f\"{'Metric':<30} {'Original':<20} {'Improved':<20} {'Change':<20}\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    original_lines = original.split('\\n')\n",
    "    improved_lines = improved.split('\\n')\n",
    "    \n",
    "    stats = [\n",
    "        (\"Length (characters)\", len(original), len(improved)),\n",
    "        (\"Length (words)\", len(original.split()), len(improved.split())),\n",
    "        (\"Lines\", len(original_lines), len(improved_lines)),\n",
    "        (\"Has numbered list\", bool(re.search(r'\\d+\\.', original)), bool(re.search(r'\\d+\\.', improved))),\n",
    "        (\"Has headers\", bool(re.search(r'^#+', original, re.MULTILINE)), bool(re.search(r'^#+', improved, re.MULTILINE))),\n",
    "        (\"Has examples\", 'example' in original.lower(), 'example' in improved.lower()),\n",
    "        (\"Has role/persona\", bool(re.search(r'you are', original.lower())), bool(re.search(r'you are', improved.lower()))),\n",
    "    ]\n",
    "    \n",
    "    for metric, orig_val, impr_val in stats:\n",
    "        if isinstance(orig_val, bool):\n",
    "            orig_str = \"‚úÖ\" if orig_val else \"‚ùå\"\n",
    "            impr_str = \"‚úÖ\" if impr_val else \"‚ùå\"\n",
    "            change_str = \"Added\" if (not orig_val and impr_val) else \"Same\"\n",
    "        else:\n",
    "            orig_str = str(orig_val)\n",
    "            impr_str = str(impr_val)\n",
    "            change = impr_val - orig_val\n",
    "            change_str = f\"+{change}\" if change > 0 else str(change)\n",
    "        \n",
    "        print(f\"{metric:<30} {orig_str:<20} {impr_str:<20} {change_str:<20}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"CHANGES EXPLANATION\")\n",
    "    print(\"=\"*100)\n",
    "    print(changes)\n",
    "\n",
    "\n",
    "# Test comparison\n",
    "if 'result' in globals() and 'improvement_result' in globals():\n",
    "    compare_prompts(\n",
    "        result['prompt'],\n",
    "        improvement_result['improved_prompt'],\n",
    "        improvement_result['change_explanations']\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
