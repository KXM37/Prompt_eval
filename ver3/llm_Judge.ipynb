{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67e9f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Dict, List, Tuple\n",
    "import anthropic\n",
    "\n",
    "class LLMJudgeEvaluator:\n",
    "    \"\"\"\n",
    "    LLM-based evaluator for subjective metrics.\n",
    "    Uses Claude API to score prompts on complex criteria.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, metric_config: dict, config: dict, api_key: str = None):\n",
    "        self.metric_config = metric_config\n",
    "        self.config = config\n",
    "        self.client = anthropic.Anthropic(api_key=api_key) if api_key else None\n",
    "    \n",
    "    def evaluate_all(self, prompt: str, below_threshold_only: bool = False, \n",
    "                     below_threshold_metrics: List[str] = None) -> Dict[str, Dict]:\n",
    "        \"\"\"\n",
    "        Evaluate LLM judge metrics.\n",
    "        \n",
    "        Args:\n",
    "            prompt: The prompt to evaluate\n",
    "            below_threshold_only: If True, only evaluate specified metrics\n",
    "            below_threshold_metrics: List of metric names to re-evaluate\n",
    "        \"\"\"\n",
    "        # Get LLM judge metrics\n",
    "        llm_metrics = {\n",
    "            name: config for name, config in self.metric_config.items()\n",
    "            if config['type'] == 'llm_judge'\n",
    "        }\n",
    "        \n",
    "        # Filter to below-threshold metrics if requested\n",
    "        if below_threshold_only and below_threshold_metrics:\n",
    "            llm_metrics = {\n",
    "                name: config for name, config in llm_metrics.items()\n",
    "                if name in below_threshold_metrics\n",
    "            }\n",
    "        \n",
    "        # Batch metrics for evaluation\n",
    "        metric_names = list(llm_metrics.keys())\n",
    "        batches = self._create_batches(metric_names, self.config['llm_batch_size'])\n",
    "        \n",
    "        print(f\"Evaluating {len(metric_names)} LLM judge metrics in {len(batches)} batches...\")\n",
    "        \n",
    "        # Evaluate each batch\n",
    "        all_results = {}\n",
    "        for i, batch in enumerate(batches, 1):\n",
    "            print(f\"  Processing batch {i}/{len(batches)}...\")\n",
    "            batch_results = self._evaluate_batch(prompt, batch, llm_metrics)\n",
    "            all_results.update(batch_results)\n",
    "        \n",
    "        return all_results\n",
    "    \n",
    "    def _create_batches(self, items: List[str], batch_size: int) -> List[List[str]]:\n",
    "        \"\"\"Split items into batches.\"\"\"\n",
    "        return [items[i:i + batch_size] for i in range(0, len(items), batch_size)]\n",
    "    \n",
    "    def _evaluate_batch(self, prompt: str, metric_names: List[str], \n",
    "                       llm_metrics: Dict) -> Dict[str, Dict]:\n",
    "        \"\"\"Evaluate a batch of metrics using Claude API.\"\"\"\n",
    "        \n",
    "        # Build evaluation prompt\n",
    "        eval_prompt = self._build_evaluation_prompt(prompt, metric_names, llm_metrics)\n",
    "        \n",
    "        # Call Claude API\n",
    "        try:\n",
    "            response = self.client.messages.create(\n",
    "                model=\"claude-sonnet-4-20250514\",\n",
    "                max_tokens=4000,\n",
    "                temperature=0,\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": eval_prompt}\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            response_text = response.content[0].text\n",
    "            \n",
    "            # Parse response\n",
    "            results = self._parse_evaluation_response(response_text, metric_names, llm_metrics)\n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error calling Claude API: {e}\")\n",
    "            # Return default scores\n",
    "            return {\n",
    "                name: {\n",
    "                    'score': 0,\n",
    "                    'max_points': llm_metrics[name]['max_points'],\n",
    "                    'threshold': llm_metrics[name]['threshold'],\n",
    "                    'below_threshold': True,\n",
    "                    'justification': f\"Error during evaluation: {str(e)}\",\n",
    "                    'type': 'llm_judge'\n",
    "                }\n",
    "                for name in metric_names\n",
    "            }\n",
    "    \n",
    "    def _build_evaluation_prompt(self, prompt: str, metric_names: List[str], \n",
    "                                 llm_metrics: Dict) -> str:\n",
    "        \"\"\"Build the evaluation prompt for Claude.\"\"\"\n",
    "        \n",
    "        metrics_description = []\n",
    "        for name in metric_names:\n",
    "            config = llm_metrics[name]\n",
    "            metrics_description.append(\n",
    "                f\"**{name}** (max: {config['max_points']} points)\\n\"\n",
    "                f\"Description: {config['description']}\\n\"\n",
    "                f\"Threshold: {config['threshold']} points (70%)\"\n",
    "            )\n",
    "        \n",
    "        eval_prompt = f\"\"\"You are an expert prompt engineer evaluating the quality of a prompt across multiple dimensions.\n",
    "\n",
    "PROMPT TO EVALUATE:\n",
    "```\n",
    "{prompt}\n",
    "```\n",
    "\n",
    "YOUR TASK:\n",
    "Evaluate the above prompt on the following {len(metric_names)} metrics. For each metric, provide:\n",
    "1. A score from 0 to the maximum points (be precise, use decimals)\n",
    "2. A brief justification (1-2 sentences)\n",
    "\n",
    "METRICS TO EVALUATE:\n",
    "\n",
    "{chr(10).join(metrics_description)}\n",
    "\n",
    "SCORING GUIDELINES:\n",
    "- 0 points: Metric completely absent or fails entirely\n",
    "- 25% of max: Metric barely present, minimal effort\n",
    "- 50% of max: Metric partially present, moderate effort\n",
    "- 75% of max: Metric well-implemented, meets threshold\n",
    "- 100% of max: Metric excellently implemented, exemplary\n",
    "\n",
    "IMPORTANT:\n",
    "- Be objective and consistent\n",
    "- Consider the prompt's actual content, not potential\n",
    "- A simple prompt may legitimately score low on some metrics\n",
    "- Use decimals for precision (e.g., 3.2, 4.5)\n",
    "\n",
    "OUTPUT FORMAT:\n",
    "Respond with a JSON object where each key is the metric name and the value is an object with \"score\" and \"justification\":\n",
    "```json\n",
    "{{\n",
    "  \"Metric Name 1\": {{\n",
    "    \"score\": 3.5,\n",
    "    \"justification\": \"Brief explanation of the score.\"\n",
    "  }},\n",
    "  \"Metric Name 2\": {{\n",
    "    \"score\": 4.0,\n",
    "    \"justification\": \"Brief explanation of the score.\"\n",
    "  }}\n",
    "}}\n",
    "```\n",
    "\n",
    "Provide ONLY the JSON object, no additional text.\"\"\"\n",
    "        \n",
    "        return eval_prompt\n",
    "    \n",
    "    def _parse_evaluation_response(self, response_text: str, metric_names: List[str],\n",
    "                                   llm_metrics: Dict) -> Dict[str, Dict]:\n",
    "        \"\"\"Parse Claude's evaluation response.\"\"\"\n",
    "        \n",
    "        # Extract JSON from response\n",
    "        try:\n",
    "            # Find JSON in response (handle markdown code blocks)\n",
    "            json_match = re.search(r'```json\\s*(\\{[\\s\\S]*?\\})\\s*```', response_text)\n",
    "            if json_match:\n",
    "                json_str = json_match.group(1)\n",
    "            else:\n",
    "                # Try to find raw JSON\n",
    "                json_match = re.search(r'\\{[\\s\\S]*\\}', response_text)\n",
    "                if json_match:\n",
    "                    json_str = json_match.group(0)\n",
    "                else:\n",
    "                    raise ValueError(\"No JSON found in response\")\n",
    "            \n",
    "            evaluation_data = json.loads(json_str)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing JSON response: {e}\")\n",
    "            print(f\"Response was: {response_text[:500]}\")\n",
    "            # Return default scores\n",
    "            return {\n",
    "                name: {\n",
    "                    'score': 0,\n",
    "                    'max_points': llm_metrics[name]['max_points'],\n",
    "                    'threshold': llm_metrics[name]['threshold'],\n",
    "                    'below_threshold': True,\n",
    "                    'justification': \"Failed to parse evaluation response\",\n",
    "                    'type': 'llm_judge'\n",
    "                }\n",
    "                for name in metric_names\n",
    "            }\n",
    "        \n",
    "        # Build results\n",
    "        results = {}\n",
    "        for name in metric_names:\n",
    "            config = llm_metrics[name]\n",
    "            \n",
    "            if name in evaluation_data:\n",
    "                eval_result = evaluation_data[name]\n",
    "                score = float(eval_result.get('score', 0))\n",
    "                justification = eval_result.get('justification', 'No justification provided')\n",
    "            else:\n",
    "                score = 0\n",
    "                justification = f\"Metric not found in evaluation response\"\n",
    "            \n",
    "            # Cap score at max_points\n",
    "            score = min(score, config['max_points'])\n",
    "            \n",
    "            results[name] = {\n",
    "                'score': round(score, 2),\n",
    "                'max_points': config['max_points'],\n",
    "                'threshold': config['threshold'],\n",
    "                'below_threshold': score < config['threshold'],\n",
    "                'justification': justification,\n",
    "                'type': 'llm_judge'\n",
    "            }\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "print(\"✅ LLM Judge Evaluator Created!\")\n",
    "print(\"\\nLLM Judge Metrics (27 total):\")\n",
    "llm_metrics = [name for name, config in METRIC_CONFIG.items() if config['type'] == 'llm_judge']\n",
    "print(f\"Total: {len(llm_metrics)} metrics\")\n",
    "print(f\"Batch size: {CONFIG['llm_batch_size']} metrics per API call\")\n",
    "print(f\"Estimated API calls for full evaluation: {len(llm_metrics) // CONFIG['llm_batch_size'] + (1 if len(llm_metrics) % CONFIG['llm_batch_size'] else 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639812e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test setup\n",
    "test_prompt_simple = \"Write a blog post about AI.\"\n",
    "\n",
    "test_prompt_complex = \"\"\"\n",
    "You are an expert technical writer with 10 years of experience in AI/ML documentation.\n",
    "\n",
    "**Task**: Create a comprehensive guide comparing transformer architectures for a technical audience.\n",
    "\n",
    "**Requirements**:\n",
    "1. Start with foundational concepts\n",
    "2. Compare at least 3 architectures (BERT, GPT, T5)\n",
    "3. Include code examples where relevant\n",
    "4. Cite recent research (2023-2024)\n",
    "\n",
    "**Output Format**:\n",
    "- Use clear section headers\n",
    "- Include diagrams descriptions\n",
    "- Provide a summary table\n",
    "\n",
    "**Constraints**:\n",
    "- Keep explanations accessible but technically accurate\n",
    "- Validate claims against established literature\n",
    "- If uncertain about recent developments, clearly state limitations\n",
    "\n",
    "Please think step-by-step and validate your technical claims.\n",
    "\"\"\"\n",
    "\n",
    "# Function to test with API key\n",
    "def test_llm_judge(api_key: str = None):\n",
    "    \"\"\"Test the LLM Judge Evaluator.\"\"\"\n",
    "    \n",
    "    if not api_key:\n",
    "        print(\"⚠️  API Key Required\")\n",
    "        print(\"To test the LLM Judge, provide your Anthropic API key:\")\n",
    "        print(\"  evaluator = LLMJudgeEvaluator(METRIC_CONFIG, CONFIG, api_key='your-key-here')\")\n",
    "        print(\"  results = evaluator.evaluate_all(test_prompt_complex)\")\n",
    "        return\n",
    "    \n",
    "    # Initialize with API key\n",
    "    evaluator = LLMJudgeEvaluator(METRIC_CONFIG, CONFIG, api_key=api_key)\n",
    "    \n",
    "    print(\"Testing LLM Judge Evaluator...\")\n",
    "    print(f\"\\nPrompt: {test_prompt_complex[:100]}...\\n\")\n",
    "    \n",
    "    # Evaluate\n",
    "    results = evaluator.evaluate_all(test_prompt_complex)\n",
    "    \n",
    "    # Display results\n",
    "    total_score = sum(r['score'] for r in results.values())\n",
    "    total_possible = sum(r['max_points'] for r in results.values())\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"LLM JUDGE RESULTS\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Total Score: {total_score:.2f} / {total_possible:.2f} ({total_score/total_possible*100:.1f}%)\")\n",
    "    print(f\"\\nMetric Breakdown:\\n\")\n",
    "    \n",
    "    # Group by category\n",
    "    by_group = {}\n",
    "    for metric_name, result in results.items():\n",
    "        group = METRIC_CONFIG[metric_name]['group']\n",
    "        if group not in by_group:\n",
    "            by_group[group] = []\n",
    "        by_group[group].append((metric_name, result))\n",
    "    \n",
    "    # Display by group\n",
    "    for group_name, metrics in by_group.items():\n",
    "        group_score = sum(r['score'] for _, r in metrics)\n",
    "        group_max = sum(r['max_points'] for _, r in metrics)\n",
    "        \n",
    "        print(f\"\\n{group_name.upper().replace('_', ' ')}: {group_score:.2f}/{group_max:.2f}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        for metric_name, result in metrics:\n",
    "            status = \"✅\" if not result['below_threshold'] else \"❌\"\n",
    "            print(f\"{status} {metric_name}: {result['score']:.2f}/{result['max_points']:.2f}\")\n",
    "            print(f\"   {result['justification'][:100]}...\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Show test instructions\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEST INSTRUCTIONS\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "To test the LLM Judge Evaluator, you need an Anthropic API key.\n",
    "\n",
    "Get your API key from: https://console.anthropic.com/\n",
    "\n",
    "Then run:\n",
    "    api_key = \"sk-ant-...\"  # Your API key\n",
    "    results = test_llm_judge(api_key)\n",
    "\n",
    "Or initialize directly:\n",
    "    evaluator = LLMJudgeEvaluator(METRIC_CONFIG, CONFIG, api_key=api_key)\n",
    "    results = evaluator.evaluate_all(test_prompt_complex)\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
