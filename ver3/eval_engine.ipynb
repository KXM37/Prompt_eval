{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d2783c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "class PromptEvaluator:\n",
    "    \"\"\"\n",
    "    Unified evaluator combining custom functions and LLM judge.\n",
    "    Provides comprehensive prompt evaluation on 0-150 scale.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, metric_config: dict, config: dict, api_key: str = None):\n",
    "        self.metric_config = metric_config\n",
    "        self.config = config\n",
    "        \n",
    "        # Initialize sub-evaluators\n",
    "        self.custom_evaluator = CustomMetricEvaluator(metric_config)\n",
    "        self.llm_evaluator = LLMJudgeEvaluator(metric_config, config, api_key)\n",
    "    \n",
    "    def evaluate(self, prompt: str, first_run: bool = True, \n",
    "                 below_threshold_metrics: List[str] = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Evaluate a prompt across all metrics.\n",
    "        \n",
    "        Args:\n",
    "            prompt: The prompt text to evaluate\n",
    "            first_run: If True, evaluate all metrics. If False, only re-evaluate specified metrics\n",
    "            below_threshold_metrics: List of metric names to re-evaluate (used when first_run=False)\n",
    "        \n",
    "        Returns:\n",
    "            Comprehensive evaluation results with scores and analysis\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"EVALUATING PROMPT\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(f\"First run: {first_run}\")\n",
    "        if not first_run and below_threshold_metrics:\n",
    "            print(f\"Re-evaluating {len(below_threshold_metrics)} metrics\")\n",
    "        print()\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        # Step 1: Custom Function Metrics (always evaluate all)\n",
    "        print(\"Step 1: Evaluating Custom Function Metrics...\")\n",
    "        custom_results = self.custom_evaluator.evaluate_all(prompt)\n",
    "        results.update(custom_results)\n",
    "        custom_score = sum(r['score'] for r in custom_results.values())\n",
    "        custom_max = sum(r['max_points'] for r in custom_results.values())\n",
    "        print(f\"✅ Custom metrics complete: {custom_score:.2f}/{custom_max:.2f}\")\n",
    "        \n",
    "        # Step 2: LLM Judge Metrics\n",
    "        print(\"\\nStep 2: Evaluating LLM Judge Metrics...\")\n",
    "        \n",
    "        if first_run:\n",
    "            # First run: evaluate all LLM metrics\n",
    "            llm_results = self.llm_evaluator.evaluate_all(prompt, \n",
    "                                                          below_threshold_only=False)\n",
    "        else:\n",
    "            # Subsequent runs: only evaluate below-threshold LLM metrics\n",
    "            llm_metrics_to_eval = [m for m in below_threshold_metrics \n",
    "                                   if self.metric_config[m]['type'] == 'llm_judge']\n",
    "            \n",
    "            if llm_metrics_to_eval:\n",
    "                llm_results = self.llm_evaluator.evaluate_all(prompt,\n",
    "                                                              below_threshold_only=True,\n",
    "                                                              below_threshold_metrics=llm_metrics_to_eval)\n",
    "            else:\n",
    "                llm_results = {}\n",
    "        \n",
    "        results.update(llm_results)\n",
    "        llm_score = sum(r['score'] for r in llm_results.values())\n",
    "        llm_max = sum(r['max_points'] for r in llm_results.values())\n",
    "        print(f\"✅ LLM judge complete: {llm_score:.2f}/{llm_max:.2f}\")\n",
    "        \n",
    "        # Step 3: Calculate totals and analysis\n",
    "        evaluation_result = self._compile_results(prompt, results, first_run)\n",
    "        \n",
    "        return evaluation_result\n",
    "    \n",
    "    def _compile_results(self, prompt: str, metrics_results: Dict, \n",
    "                        first_run: bool) -> Dict:\n",
    "        \"\"\"Compile comprehensive evaluation results.\"\"\"\n",
    "        \n",
    "        # Calculate total score\n",
    "        total_score = sum(r['score'] for r in metrics_results.values())\n",
    "        total_max = sum(r['max_points'] for r in metrics_results.values())\n",
    "        percentage = (total_score / total_max * 100) if total_max > 0 else 0\n",
    "        \n",
    "        # Identify below-threshold metrics\n",
    "        below_threshold = [\n",
    "            name for name, result in metrics_results.items()\n",
    "            if result['below_threshold']\n",
    "        ]\n",
    "        \n",
    "        # Calculate group scores\n",
    "        group_scores = {}\n",
    "        for group_name in GROUP_TOTALS.keys():\n",
    "            group_metrics = {\n",
    "                name: result for name, result in metrics_results.items()\n",
    "                if self.metric_config[name]['group'] == group_name\n",
    "            }\n",
    "            if group_metrics:\n",
    "                group_score = sum(r['score'] for r in group_metrics.values())\n",
    "                group_max = sum(r['max_points'] for r in group_metrics.values())\n",
    "                group_scores[group_name] = {\n",
    "                    'score': round(group_score, 2),\n",
    "                    'max': group_max,\n",
    "                    'percentage': round(group_score / group_max * 100, 1) if group_max > 0 else 0\n",
    "                }\n",
    "        \n",
    "        # Compile result\n",
    "        result = {\n",
    "            'prompt': prompt,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'first_run': first_run,\n",
    "            'metrics': metrics_results,\n",
    "            'total_score': round(total_score, 2),\n",
    "            'total_max': total_max,\n",
    "            'percentage': round(percentage, 1),\n",
    "            'below_threshold_metrics': below_threshold,\n",
    "            'below_threshold_count': len(below_threshold),\n",
    "            'group_scores': group_scores\n",
    "        }\n",
    "        \n",
    "        # Print summary\n",
    "        self._print_summary(result)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _print_summary(self, result: Dict):\n",
    "        \"\"\"Print evaluation summary.\"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"EVALUATION SUMMARY\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"Total Score: {result['total_score']:.2f} / {result['total_max']} ({result['percentage']:.1f}%)\")\n",
    "        print(f\"Below Threshold: {result['below_threshold_count']} metrics\")\n",
    "        \n",
    "        print(f\"\\nGroup Scores:\")\n",
    "        for group_name, group_data in result['group_scores'].items():\n",
    "            status = \"✅\" if group_data['percentage'] >= 70 else \"⚠️\"\n",
    "            print(f\"  {status} {group_name.replace('_', ' ').title()}: \"\n",
    "                  f\"{group_data['score']:.2f}/{group_data['max']} ({group_data['percentage']:.1f}%)\")\n",
    "        \n",
    "        if result['below_threshold_metrics']:\n",
    "            print(f\"\\nMetrics Below Threshold ({len(result['below_threshold_metrics'])}):\")\n",
    "            for metric in result['below_threshold_metrics'][:10]:  # Show first 10\n",
    "                metric_result = result['metrics'][metric]\n",
    "                print(f\"  ❌ {metric}: {metric_result['score']:.2f}/{metric_result['max_points']:.2f}\")\n",
    "            if len(result['below_threshold_metrics']) > 10:\n",
    "                print(f\"  ... and {len(result['below_threshold_metrics']) - 10} more\")\n",
    "    \n",
    "    def generate_detailed_report(self, result: Dict) -> str:\n",
    "        \"\"\"Generate a detailed text report of evaluation results.\"\"\"\n",
    "        \n",
    "        report_lines = []\n",
    "        report_lines.append(\"=\" * 80)\n",
    "        report_lines.append(\"PROMPT EVALUATION REPORT\")\n",
    "        report_lines.append(\"=\" * 80)\n",
    "        report_lines.append(f\"Timestamp: {result['timestamp']}\")\n",
    "        report_lines.append(f\"Total Score: {result['total_score']:.2f} / {result['total_max']} ({result['percentage']:.1f}%)\")\n",
    "        report_lines.append(\"\")\n",
    "        \n",
    "        # Prompt\n",
    "        report_lines.append(\"PROMPT:\")\n",
    "        report_lines.append(\"-\" * 80)\n",
    "        report_lines.append(result['prompt'])\n",
    "        report_lines.append(\"\")\n",
    "        \n",
    "        # Group-by-group breakdown\n",
    "        for group_name in GROUP_TOTALS.keys():\n",
    "            group_data = result['group_scores'].get(group_name)\n",
    "            if not group_data:\n",
    "                continue\n",
    "            \n",
    "            report_lines.append(\"\")\n",
    "            report_lines.append(f\"{group_name.upper().replace('_', ' ')}\")\n",
    "            report_lines.append(f\"Score: {group_data['score']:.2f}/{group_data['max']} ({group_data['percentage']:.1f}%)\")\n",
    "            report_lines.append(\"-\" * 80)\n",
    "            \n",
    "            # Get metrics in this group\n",
    "            group_metrics = {\n",
    "                name: result['metrics'][name] \n",
    "                for name in result['metrics']\n",
    "                if self.metric_config[name]['group'] == group_name\n",
    "            }\n",
    "            \n",
    "            for metric_name, metric_result in group_metrics.items():\n",
    "                status = \"✅\" if not metric_result['below_threshold'] else \"❌\"\n",
    "                report_lines.append(f\"\\n{status} {metric_name}\")\n",
    "                report_lines.append(f\"   Score: {metric_result['score']:.2f}/{metric_result['max_points']:.2f} \"\n",
    "                                  f\"(Threshold: {metric_result['threshold']:.2f})\")\n",
    "                \n",
    "                if metric_result['type'] == 'llm_judge':\n",
    "                    report_lines.append(f\"   Justification: {metric_result['justification']}\")\n",
    "                else:\n",
    "                    report_lines.append(f\"   Evidence: {metric_result['evidence']}\")\n",
    "        \n",
    "        # Summary of improvements needed\n",
    "        report_lines.append(\"\")\n",
    "        report_lines.append(\"=\" * 80)\n",
    "        report_lines.append(\"IMPROVEMENTS NEEDED\")\n",
    "        report_lines.append(\"=\" * 80)\n",
    "        \n",
    "        if result['below_threshold_metrics']:\n",
    "            report_lines.append(f\"\\n{len(result['below_threshold_metrics'])} metrics are below threshold:\\n\")\n",
    "            for metric in result['below_threshold_metrics']:\n",
    "                metric_result = result['metrics'][metric]\n",
    "                gap = metric_result['threshold'] - metric_result['score']\n",
    "                report_lines.append(f\"  • {metric}: {metric_result['score']:.2f}/{metric_result['max_points']:.2f} \"\n",
    "                                  f\"(need +{gap:.2f} points)\")\n",
    "        else:\n",
    "            report_lines.append(\"\\n✅ All metrics meet or exceed threshold!\")\n",
    "        \n",
    "        return \"\\n\".join(report_lines)\n",
    "\n",
    "\n",
    "print(\"✅ Full Evaluator Pipeline Created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256c61ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prompts\n",
    "test_prompts = {\n",
    "    \"minimal\": \"Write about AI\",\n",
    "    \n",
    "    \"moderate\": \"\"\"\n",
    "Write a blog post about artificial intelligence for beginners.\n",
    "Include:\n",
    "- What is AI?\n",
    "- Current applications\n",
    "- Future trends\n",
    "\n",
    "Keep it simple and engaging.\n",
    "\"\"\",\n",
    "    \n",
    "    \"comprehensive\": \"\"\"\n",
    "# Task: Technical Article on Transformer Architecture\n",
    "\n",
    "You are an expert ML engineer writing for a technical audience of software engineers and data scientists.\n",
    "\n",
    "## Objective\n",
    "Create a comprehensive technical guide comparing different transformer architectures (BERT, GPT, T5) for natural language processing tasks.\n",
    "\n",
    "## Requirements\n",
    "1. Start with transformer fundamentals (attention mechanism)\n",
    "2. Compare architectural differences between BERT, GPT, and T5\n",
    "3. Provide code examples for each architecture\n",
    "4. Discuss use cases and trade-offs\n",
    "5. Include performance benchmarks (cite sources)\n",
    "\n",
    "## Output Format\n",
    "- Use clear section headers\n",
    "- Include code blocks with explanations\n",
    "- Provide a comparison table\n",
    "- Add a \"Further Reading\" section\n",
    "\n",
    "## Constraints\n",
    "- Keep technical accuracy paramount\n",
    "- Validate all claims against published research\n",
    "- If uncertain about recent developments, state limitations clearly\n",
    "- Estimate reading time for the article\n",
    "\n",
    "## Process\n",
    "1. Think step-by-step through the architecture comparisons\n",
    "2. Verify technical claims\n",
    "3. Iterate on clarity and completeness\n",
    "4. Self-review for accuracy\n",
    "\n",
    "Remember our previous discussion about balancing depth with accessibility for intermediate practitioners.\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "# Test function\n",
    "def test_full_evaluator(api_key: str = None, use_mock: bool = True):\n",
    "    \"\"\"Test the full evaluator pipeline.\"\"\"\n",
    "    \n",
    "    # Choose which prompt to test\n",
    "    test_name = \"comprehensive\"\n",
    "    test_prompt = test_prompts[test_name]\n",
    "    \n",
    "    print(f\"Testing Full Evaluator with '{test_name}' prompt\")\n",
    "    print(f\"Using {'MOCK' if use_mock else 'REAL'} LLM evaluation\")\n",
    "    print()\n",
    "    \n",
    "    # Initialize evaluator\n",
    "    if use_mock:\n",
    "        # Temporarily replace LLM evaluator with mock version\n",
    "        evaluator = PromptEvaluator(METRIC_CONFIG, CONFIG, api_key=None)\n",
    "        \n",
    "        # Override the LLM evaluator's evaluate_all method with mock\n",
    "        original_evaluate = evaluator.llm_evaluator.evaluate_all\n",
    "        \n",
    "        def mock_evaluate_all(prompt, below_threshold_only=False, below_threshold_metrics=None):\n",
    "            llm_metrics = {\n",
    "                name: config for name, config in METRIC_CONFIG.items()\n",
    "                if config['type'] == 'llm_judge'\n",
    "            }\n",
    "            if below_threshold_only and below_threshold_metrics:\n",
    "                llm_metrics = {\n",
    "                    name: config for name, config in llm_metrics.items()\n",
    "                    if name in below_threshold_metrics\n",
    "                }\n",
    "            return mock_llm_evaluation(prompt, list(llm_metrics.keys()))\n",
    "        \n",
    "        evaluator.llm_evaluator.evaluate_all = mock_evaluate_all\n",
    "    else:\n",
    "        if not api_key:\n",
    "            print(\"⚠️  API key required for real LLM evaluation\")\n",
    "            return None\n",
    "        evaluator = PromptEvaluator(METRIC_CONFIG, CONFIG, api_key=api_key)\n",
    "    \n",
    "    # Run evaluation\n",
    "    result = evaluator.evaluate(test_prompt, first_run=True)\n",
    "    \n",
    "    # Generate detailed report\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"GENERATING DETAILED REPORT\")\n",
    "    print(\"=\"*80)\n",
    "    report = evaluator.generate_detailed_report(result)\n",
    "    \n",
    "    # Show first part of report\n",
    "    report_lines = report.split('\\n')\n",
    "    print('\\n'.join(report_lines[:50]))  # Show first 50 lines\n",
    "    if len(report_lines) > 50:\n",
    "        print(f\"\\n... ({len(report_lines) - 50} more lines) ...\")\n",
    "    \n",
    "    return result, report\n",
    "\n",
    "# Run test\n",
    "print(\"=\"*80)\n",
    "print(\"RUNNING FULL EVALUATOR TEST\")\n",
    "print(\"=\"*80)\n",
    "result, report = test_full_evaluator(use_mock=True)\n",
    "\n",
    "# Save result for next steps\n",
    "if result:\n",
    "    print(f\"\\n✅ Evaluation complete!\")\n",
    "    print(f\"   Result stored in 'result' variable\")\n",
    "    print(f\"   Full report stored in 'report' variable\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
