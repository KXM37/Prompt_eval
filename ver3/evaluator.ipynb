{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417ffc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "class CustomMetricEvaluator:\n",
    "    \"\"\"\n",
    "    Custom function-based metrics using pattern matching and keyword detection.\n",
    "    Each function returns (score, evidence) tuple.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, metric_config: dict):\n",
    "        self.metric_config = metric_config\n",
    "    \n",
    "    def evaluate_all(self, prompt: str) -> Dict[str, Dict]:\n",
    "        \"\"\"Evaluate all custom function metrics.\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        # Get all custom function metrics\n",
    "        custom_metrics = {\n",
    "            name: config for name, config in self.metric_config.items()\n",
    "            if config['type'] == 'custom_function'\n",
    "        }\n",
    "        \n",
    "        for metric_name, config in custom_metrics.items():\n",
    "            score, evidence = self._evaluate_metric(metric_name, prompt, config)\n",
    "            results[metric_name] = {\n",
    "                'score': score,\n",
    "                'max_points': config['max_points'],\n",
    "                'threshold': config['threshold'],\n",
    "                'below_threshold': score < config['threshold'],\n",
    "                'evidence': evidence,\n",
    "                'type': 'custom_function'\n",
    "            }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _evaluate_metric(self, metric_name: str, prompt: str, config: dict) -> Tuple[float, str]:\n",
    "        \"\"\"Route to specific evaluation function.\"\"\"\n",
    "        evaluators = {\n",
    "            \"Structured / Numbered Instructions\": self._eval_structured_instructions,\n",
    "            \"Examples or Demonstrations\": self._eval_examples,\n",
    "            \"Use of Role or Persona\": self._eval_role_persona,\n",
    "            \"Audience Specification\": self._eval_audience,\n",
    "            \"Output Validation Hooks\": self._eval_validation_hooks,\n",
    "            \"Time/Effort Estimation Request\": self._eval_time_estimation,\n",
    "            \"Self-Repair Loops\": self._eval_self_repair,\n",
    "            \"Memory Anchoring\": self._eval_memory_anchoring,\n",
    "            \"Calibration Requests\": self._eval_calibration,\n",
    "            \"Comparison Requests\": self._eval_comparison\n",
    "        }\n",
    "        \n",
    "        evaluator = evaluators.get(metric_name)\n",
    "        if evaluator:\n",
    "            return evaluator(prompt, config['max_points'])\n",
    "        return 0, \"Evaluator not found\"\n",
    "    \n",
    "    # ===========================================\n",
    "    # INDIVIDUAL METRIC EVALUATORS\n",
    "    # ===========================================\n",
    "    \n",
    "    def _eval_structured_instructions(self, prompt: str, max_points: float) -> Tuple[float, str]:\n",
    "        \"\"\"Detect structured formatting (bullets, numbers, sections).\"\"\"\n",
    "        evidence_parts = []\n",
    "        score = 0\n",
    "        \n",
    "        # Check for numbered lists (1. 2. or 1) 2) etc.)\n",
    "        numbered_pattern = r'(?:^|\\n)\\s*\\d+[\\.)]\\s+'\n",
    "        numbered_matches = len(re.findall(numbered_pattern, prompt))\n",
    "        if numbered_matches >= 3:\n",
    "            score += max_points * 0.4\n",
    "            evidence_parts.append(f\"Numbered list with {numbered_matches} items\")\n",
    "        \n",
    "        # Check for bullet points (-, *, •)\n",
    "        bullet_pattern = r'(?:^|\\n)\\s*[-*•]\\s+'\n",
    "        bullet_matches = len(re.findall(bullet_pattern, prompt))\n",
    "        if bullet_matches >= 3:\n",
    "            score += max_points * 0.3\n",
    "            evidence_parts.append(f\"Bullet points with {bullet_matches} items\")\n",
    "        \n",
    "        # Check for markdown headers (# ## ###)\n",
    "        header_pattern = r'(?:^|\\n)#{1,6}\\s+.+$'\n",
    "        header_matches = len(re.findall(header_pattern, prompt, re.MULTILINE))\n",
    "        if header_matches >= 2:\n",
    "            score += max_points * 0.3\n",
    "            evidence_parts.append(f\"{header_matches} section headers\")\n",
    "        \n",
    "        # Cap at max_points\n",
    "        score = min(score, max_points)\n",
    "        \n",
    "        evidence = \"; \".join(evidence_parts) if evidence_parts else \"No structured formatting detected\"\n",
    "        return score, evidence\n",
    "    \n",
    "    def _eval_examples(self, prompt: str, max_points: float) -> Tuple[float, str]:\n",
    "        \"\"\"Detect examples or demonstrations.\"\"\"\n",
    "        evidence_parts = []\n",
    "        score = 0\n",
    "        \n",
    "        # Keywords indicating examples\n",
    "        example_keywords = [\n",
    "            r'\\bexample[s]?\\b', r'\\bfor instance\\b', r'\\bsuch as\\b',\n",
    "            r'\\be\\.g\\.\\b', r'\\blike\\b.*\\bthis\\b', r'\\bdemonstration\\b',\n",
    "            r'\\bsample\\b', r'\\billustration\\b'\n",
    "        ]\n",
    "        \n",
    "        keyword_matches = 0\n",
    "        for pattern in example_keywords:\n",
    "            if re.search(pattern, prompt, re.IGNORECASE):\n",
    "                keyword_matches += 1\n",
    "        \n",
    "        if keyword_matches > 0:\n",
    "            score += max_points * 0.3\n",
    "            evidence_parts.append(f\"{keyword_matches} example indicator keywords\")\n",
    "        \n",
    "        # Code blocks (``` or indented code)\n",
    "        code_block_pattern = r'```[\\s\\S]*?```|(?:^|\\n)    .+(?:\\n    .+)*'\n",
    "        code_blocks = len(re.findall(code_block_pattern, prompt))\n",
    "        if code_blocks > 0:\n",
    "            score += max_points * 0.4\n",
    "            evidence_parts.append(f\"{code_blocks} code block(s)\")\n",
    "        \n",
    "        # Quoted examples (\"...\" or '...')\n",
    "        quote_pattern = r'[\"\\'](?:[^\"\\']{20,})[\"\\']'\n",
    "        quotes = len(re.findall(quote_pattern, prompt))\n",
    "        if quotes > 0:\n",
    "            score += max_points * 0.3\n",
    "            evidence_parts.append(f\"{quotes} quoted example(s)\")\n",
    "        \n",
    "        # Cap at max_points\n",
    "        score = min(score, max_points)\n",
    "        \n",
    "        evidence = \"; \".join(evidence_parts) if evidence_parts else \"No examples detected\"\n",
    "        return score, evidence\n",
    "    \n",
    "    def _eval_role_persona(self, prompt: str, max_points: float) -> Tuple[float, str]:\n",
    "        \"\"\"Detect role or persona assignment.\"\"\"\n",
    "        evidence_parts = []\n",
    "        score = 0\n",
    "        \n",
    "        # Strong role indicators\n",
    "        strong_patterns = [\n",
    "            r'\\byou are (?:a |an |the )?(\\w+)',\n",
    "            r'\\bact as (?:a |an |the )?(\\w+)',\n",
    "            r'\\bassume the role of (?:a |an |the )?(\\w+)',\n",
    "            r'\\bpretend (?:to be|you are) (?:a |an |the )?(\\w+)',\n",
    "            r'\\btake on the persona of (?:a |an |the )?(\\w+)',\n",
    "            r'\\bas (?:a |an |the )?(\\w+), you'\n",
    "        ]\n",
    "        \n",
    "        for pattern in strong_patterns:\n",
    "            matches = re.findall(pattern, prompt, re.IGNORECASE)\n",
    "            if matches:\n",
    "                score = max_points  # Full points for explicit role\n",
    "                roles = [m if isinstance(m, str) else m[0] for m in matches[:3]]\n",
    "                evidence_parts.append(f\"Explicit role: {', '.join(roles)}\")\n",
    "                break\n",
    "        \n",
    "        # Weaker role indicators\n",
    "        if score == 0:\n",
    "            weak_patterns = [\n",
    "                r'\\bexpert\\b', r'\\bprofessional\\b', r'\\bspecialist\\b',\n",
    "                r'\\bconsultant\\b', r'\\badviser\\b', r'\\bassistant\\b'\n",
    "            ]\n",
    "            weak_matches = sum(1 for p in weak_patterns if re.search(p, prompt, re.IGNORECASE))\n",
    "            if weak_matches > 0:\n",
    "                score = max_points * 0.5\n",
    "                evidence_parts.append(f\"Implicit role indicators: {weak_matches} found\")\n",
    "        \n",
    "        evidence = \"; \".join(evidence_parts) if evidence_parts else \"No role or persona detected\"\n",
    "        return score, evidence\n",
    "    \n",
    "    def _eval_audience(self, prompt: str, max_points: float) -> Tuple[float, str]:\n",
    "        \"\"\"Detect audience specification.\"\"\"\n",
    "        evidence_parts = []\n",
    "        score = 0\n",
    "        \n",
    "        # Explicit audience patterns\n",
    "        audience_patterns = [\n",
    "            r'\\bfor (?:a |an |the )?(\\w+(?:\\s+\\w+)?)\\s+audience\\b',\n",
    "            r'\\btarget audience\\s*:?\\s*(\\w+)',\n",
    "            r'\\bexplain to (?:a |an |the )?(\\w+)',\n",
    "            r'\\bsuitable for (\\w+(?:\\s+\\w+)?)',\n",
    "            r'\\bwrite for (\\w+(?:\\s+\\w+)?)',\n",
    "            r'\\baddressed to (\\w+(?:\\s+\\w+)?)'\n",
    "        ]\n",
    "        \n",
    "        for pattern in audience_patterns:\n",
    "            matches = re.findall(pattern, prompt, re.IGNORECASE)\n",
    "            if matches:\n",
    "                score = max_points  # Full points for explicit audience\n",
    "                audiences = matches[:2]\n",
    "                evidence_parts.append(f\"Target audience: {', '.join(audiences)}\")\n",
    "                break\n",
    "        \n",
    "        # Implicit audience indicators\n",
    "        if score == 0:\n",
    "            implicit_patterns = [\n",
    "                r'\\bbeginners?\\b', r'\\bexperts?\\b', r'\\bstudents?\\b',\n",
    "                r'\\bchildren\\b', r'\\bprofessionals?\\b', r'\\btechnical\\b',\n",
    "                r'\\bnon-technical\\b', r'\\bgeneral public\\b', r'\\blaymen\\b'\n",
    "            ]\n",
    "            implicit_matches = []\n",
    "            for pattern in implicit_patterns:\n",
    "                match = re.search(pattern, prompt, re.IGNORECASE)\n",
    "                if match:\n",
    "                    implicit_matches.append(match.group(0))\n",
    "            \n",
    "            if implicit_matches:\n",
    "                score = max_points * 0.6\n",
    "                evidence_parts.append(f\"Implicit audience: {', '.join(implicit_matches[:2])}\")\n",
    "        \n",
    "        evidence = \"; \".join(evidence_parts) if evidence_parts else \"No audience specification detected\"\n",
    "        return score, evidence\n",
    "    \n",
    "    def _eval_validation_hooks(self, prompt: str, max_points: float) -> Tuple[float, str]:\n",
    "        \"\"\"Detect validation or verification requests.\"\"\"\n",
    "        evidence_parts = []\n",
    "        score = 0\n",
    "        \n",
    "        validation_keywords = [\n",
    "            r'\\bvalidate\\b', r'\\bverify\\b', r'\\bcheck\\b', r'\\bensure\\b',\n",
    "            r'\\bconfirm\\b', r'\\breview\\b', r'\\bdouble-check\\b',\n",
    "            r'\\bcross-check\\b', r'\\btest\\b', r'\\bproof\\b'\n",
    "        ]\n",
    "        \n",
    "        matches = []\n",
    "        for pattern in validation_keywords:\n",
    "            found = re.findall(pattern, prompt, re.IGNORECASE)\n",
    "            matches.extend(found)\n",
    "        \n",
    "        unique_matches = len(set(m.lower() for m in matches))\n",
    "        \n",
    "        if unique_matches >= 3:\n",
    "            score = max_points\n",
    "            evidence_parts.append(f\"{unique_matches} validation keywords\")\n",
    "        elif unique_matches == 2:\n",
    "            score = max_points * 0.7\n",
    "            evidence_parts.append(f\"{unique_matches} validation keywords\")\n",
    "        elif unique_matches == 1:\n",
    "            score = max_points * 0.4\n",
    "            evidence_parts.append(f\"{unique_matches} validation keyword\")\n",
    "        \n",
    "        evidence = \"; \".join(evidence_parts) if evidence_parts else \"No validation hooks detected\"\n",
    "        return score, evidence\n",
    "    \n",
    "    def _eval_time_estimation(self, prompt: str, max_points: float) -> Tuple[float, str]:\n",
    "        \"\"\"Detect time or effort estimation requests.\"\"\"\n",
    "        evidence_parts = []\n",
    "        score = 0\n",
    "        \n",
    "        time_patterns = [\n",
    "            r'\\bestimate (?:the )?time\\b', r'\\bhow long\\b',\n",
    "            r'\\btime required\\b', r'\\btime estimate\\b',\n",
    "            r'\\beffort required\\b', r'\\bestimate (?:the )?effort\\b',\n",
    "            r'\\bduration\\b', r'\\btimeline\\b'\n",
    "        ]\n",
    "        \n",
    "        for pattern in time_patterns:\n",
    "            if re.search(pattern, prompt, re.IGNORECASE):\n",
    "                score = max_points\n",
    "                evidence_parts.append(f\"Time/effort estimation requested\")\n",
    "                break\n",
    "        \n",
    "        evidence = \"; \".join(evidence_parts) if evidence_parts else \"No time/effort estimation requested\"\n",
    "        return score, evidence\n",
    "    \n",
    "    def _eval_self_repair(self, prompt: str, max_points: float) -> Tuple[float, str]:\n",
    "        \"\"\"Detect self-correction or refinement loops.\"\"\"\n",
    "        evidence_parts = []\n",
    "        score = 0\n",
    "        \n",
    "        repair_keywords = [\n",
    "            r'\\biterate\\b', r'\\brefine\\b', r'\\bimprove\\b', r'\\brevise\\b',\n",
    "            r'\\badjust\\b', r'\\boptimize\\b', r'\\benhance\\b',\n",
    "            r'\\bself-correct\\b', r'\\bfeedback loop\\b', r'\\biterative\\b'\n",
    "        ]\n",
    "        \n",
    "        matches = []\n",
    "        for pattern in repair_keywords:\n",
    "            found = re.findall(pattern, prompt, re.IGNORECASE)\n",
    "            matches.extend(found)\n",
    "        \n",
    "        unique_matches = len(set(m.lower() for m in matches))\n",
    "        \n",
    "        if unique_matches >= 3:\n",
    "            score = max_points\n",
    "            evidence_parts.append(f\"{unique_matches} self-repair indicators\")\n",
    "        elif unique_matches == 2:\n",
    "            score = max_points * 0.7\n",
    "            evidence_parts.append(f\"{unique_matches} self-repair indicators\")\n",
    "        elif unique_matches == 1:\n",
    "            score = max_points * 0.4\n",
    "            evidence_parts.append(f\"{unique_matches} self-repair indicator\")\n",
    "        \n",
    "        evidence = \"; \".join(evidence_parts) if evidence_parts else \"No self-repair loops detected\"\n",
    "        return score, evidence\n",
    "    \n",
    "    def _eval_memory_anchoring(self, prompt: str, max_points: float) -> Tuple[float, str]:\n",
    "        \"\"\"Detect memory or context anchoring.\"\"\"\n",
    "        evidence_parts = []\n",
    "        score = 0\n",
    "        \n",
    "        memory_keywords = [\n",
    "            r'\\bremember\\b', r'\\brecall\\b', r'\\bpreviously\\b',\n",
    "            r'\\bcontext from\\b', r'\\bas we discussed\\b',\n",
    "            r'\\bearlier conversation\\b', r'\\bfrom before\\b',\n",
    "            r'\\bkeep in mind\\b', r'\\bdon\\'t forget\\b'\n",
    "        ]\n",
    "        \n",
    "        matches = []\n",
    "        for pattern in memory_keywords:\n",
    "            found = re.findall(pattern, prompt, re.IGNORECASE)\n",
    "            matches.extend(found)\n",
    "        \n",
    "        unique_matches = len(set(m.lower() for m in matches))\n",
    "        \n",
    "        if unique_matches >= 2:\n",
    "            score = max_points\n",
    "            evidence_parts.append(f\"{unique_matches} memory anchoring indicators\")\n",
    "        elif unique_matches == 1:\n",
    "            score = max_points * 0.6\n",
    "            evidence_parts.append(f\"{unique_matches} memory anchoring indicator\")\n",
    "        \n",
    "        evidence = \"; \".join(evidence_parts) if evidence_parts else \"No memory anchoring detected\"\n",
    "        return score, evidence\n",
    "    \n",
    "    def _eval_calibration(self, prompt: str, max_points: float) -> Tuple[float, str]:\n",
    "        \"\"\"Detect calibration or confidence requests.\"\"\"\n",
    "        evidence_parts = []\n",
    "        score = 0\n",
    "        \n",
    "        calibration_patterns = [\n",
    "            r'\\bcalibrate\\b', r'\\bconfidence level\\b',\n",
    "            r'\\bcertainty\\b', r'\\buncertainty\\b',\n",
    "            r'\\bhow confident\\b', r'\\btune\\b', r'\\badjust\\b.*\\bconfidence\\b'\n",
    "        ]\n",
    "        \n",
    "        for pattern in calibration_patterns:\n",
    "            if re.search(pattern, prompt, re.IGNORECASE):\n",
    "                score = max_points\n",
    "                evidence_parts.append(\"Calibration/confidence request detected\")\n",
    "                break\n",
    "        \n",
    "        evidence = \"; \".join(evidence_parts) if evidence_parts else \"No calibration requests detected\"\n",
    "        return score, evidence\n",
    "    \n",
    "    def _eval_comparison(self, prompt: str, max_points: float) -> Tuple[float, str]:\n",
    "        \"\"\"Detect comparison requests.\"\"\"\n",
    "        evidence_parts = []\n",
    "        score = 0\n",
    "        \n",
    "        comparison_keywords = [\n",
    "            r'\\bcompare\\b', r'\\bvs\\.?\\b', r'\\bversus\\b',\n",
    "            r'\\bcontrast\\b', r'\\bdifference(?:s)? between\\b',\n",
    "            r'\\bsimilarit(?:y|ies)\\b', r'\\bbetter than\\b',\n",
    "            r'\\bpros and cons\\b', r'\\badvantages? (?:and|vs) disadvantages?\\b'\n",
    "        ]\n",
    "        \n",
    "        matches = []\n",
    "        for pattern in comparison_keywords:\n",
    "            found = re.findall(pattern, prompt, re.IGNORECASE)\n",
    "            matches.extend(found)\n",
    "        \n",
    "        unique_matches = len(set(m.lower() for m in matches))\n",
    "        \n",
    "        if unique_matches >= 2:\n",
    "            score = max_points\n",
    "            evidence_parts.append(f\"{unique_matches} comparison indicators\")\n",
    "        elif unique_matches == 1:\n",
    "            score = max_points * 0.6\n",
    "            evidence_parts.append(f\"{unique_matches} comparison indicator\")\n",
    "        \n",
    "        evidence = \"; \".join(evidence_parts) if evidence_parts else \"No comparison requests detected\"\n",
    "        return score, evidence\n",
    "\n",
    "\n",
    "# Test the evaluator\n",
    "print(\"✅ Custom Metric Evaluator Created!\")\n",
    "print(\"\\nCustom Function Metrics (10 total):\")\n",
    "custom_metrics = [name for name, config in METRIC_CONFIG.items() if config['type'] == 'custom_function']\n",
    "for i, metric in enumerate(custom_metrics, 1):\n",
    "    points = METRIC_CONFIG[metric]['max_points']\n",
    "    print(f\"  {i}. {metric} ({points} pts)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cc4c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize evaluator\n",
    "evaluator = CustomMetricEvaluator(METRIC_CONFIG)\n",
    "\n",
    "# Test prompts\n",
    "test_prompts = {\n",
    "    \"Basic\": \"Write a blog post about AI.\",\n",
    "    \n",
    "    \"Structured\": \"\"\"\n",
    "# Task: Write a Blog Post\n",
    "\n",
    "Please complete the following:\n",
    "1. Research AI trends\n",
    "2. Write introduction\n",
    "3. Develop main points\n",
    "4. Conclude with insights\n",
    "\n",
    "- Use clear language\n",
    "- Include examples\n",
    "- Keep it under 1000 words\n",
    "\"\"\",\n",
    "    \n",
    "    \"Rich\": \"\"\"\n",
    "You are an expert technical writer targeting software engineers.\n",
    "\n",
    "Task: Compare Python vs JavaScript for beginners.\n",
    "\n",
    "Requirements:\n",
    "1. Explain core differences\n",
    "2. Provide code examples for each\n",
    "3. Validate your claims with recent data\n",
    "4. Estimate time to learn each language\n",
    "\n",
    "Remember our previous discussion about syntax simplicity.\n",
    "Please iterate on your draft and refine based on clarity.\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "# Evaluate each test prompt\n",
    "for name, prompt in test_prompts.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TEST: {name} Prompt\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Prompt: {prompt[:100]}...\")\n",
    "    print()\n",
    "    \n",
    "    results = evaluator.evaluate_all(prompt)\n",
    "    \n",
    "    total_score = sum(r['score'] for r in results.values())\n",
    "    total_possible = sum(r['max_points'] for r in results.values())\n",
    "    \n",
    "    print(f\"Custom Function Score: {total_score:.2f} / {total_possible:.2f}\")\n",
    "    print(f\"\\nMetric Breakdown:\")\n",
    "    \n",
    "    for metric_name, result in results.items():\n",
    "        status = \"❌\" if result['below_threshold'] else \"✅\"\n",
    "        print(f\"  {status} {metric_name}: {result['score']:.2f}/{result['max_points']:.2f}\")\n",
    "        print(f\"     Evidence: {result['evidence']}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
