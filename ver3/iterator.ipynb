{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5461db2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List\n",
    "import time\n",
    "\n",
    "class PromptOptimizer:\n",
    "    \"\"\"\n",
    "    Main orchestrator for prompt evaluation and iterative improvement.\n",
    "    Tracks best prompt across all iterations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, metric_config: dict, config: dict, api_key: str = None):\n",
    "        self.metric_config = metric_config\n",
    "        self.config = config\n",
    "        \n",
    "        # Initialize components\n",
    "        self.evaluator = PromptEvaluator(metric_config, config, api_key)\n",
    "        self.improver = PromptImprover(metric_config, config, api_key)\n",
    "        \n",
    "        # Iteration tracking\n",
    "        self.history = []\n",
    "        self.best_result = None\n",
    "    \n",
    "    def optimize(self, initial_prompt: str, max_iterations: int = None,\n",
    "                 target_score: float = None, min_improvement: float = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Optimize a prompt through iterative evaluation and improvement.\n",
    "        \n",
    "        Args:\n",
    "            initial_prompt: The starting prompt to optimize\n",
    "            max_iterations: Maximum iterations (default from config)\n",
    "            target_score: Target score to reach (default from config)\n",
    "            min_improvement: Minimum improvement to continue (default from config)\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with optimization results and best prompt\n",
    "        \"\"\"\n",
    "        # Use config defaults if not specified\n",
    "        max_iterations = max_iterations or self.config['max_iterations']\n",
    "        target_score = target_score or self.config['target_score']\n",
    "        min_improvement = min_improvement or self.config['min_improvement']\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"PROMPT OPTIMIZATION STARTED\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Max Iterations: {max_iterations}\")\n",
    "        print(f\"Target Score: {target_score}/{self.config['max_score']} ({target_score/self.config['max_score']*100:.0f}%)\")\n",
    "        print(f\"Min Improvement: {min_improvement} points\")\n",
    "        print()\n",
    "        \n",
    "        # Reset tracking\n",
    "        self.history = []\n",
    "        self.best_result = None\n",
    "        \n",
    "        current_prompt = initial_prompt\n",
    "        iteration = 0\n",
    "        \n",
    "        # Main optimization loop\n",
    "        while iteration < max_iterations:\n",
    "            iteration += 1\n",
    "            version = f\"v{iteration}\"\n",
    "            \n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"ITERATION {iteration}/{max_iterations} ({version})\")\n",
    "            print(f\"{'='*80}\")\n",
    "            \n",
    "            # Evaluate current prompt\n",
    "            is_first_run = (iteration == 1)\n",
    "            \n",
    "            if is_first_run:\n",
    "                # First iteration: evaluate all metrics\n",
    "                eval_result = self.evaluator.evaluate(current_prompt, first_run=True)\n",
    "            else:\n",
    "                # Subsequent iterations: only re-evaluate below-threshold metrics\n",
    "                previous_below_threshold = self.history[-1]['evaluation']['below_threshold_metrics']\n",
    "                eval_result = self.evaluator.evaluate(\n",
    "                    current_prompt, \n",
    "                    first_run=False,\n",
    "                    below_threshold_metrics=previous_below_threshold\n",
    "                )\n",
    "            \n",
    "            # Store iteration data\n",
    "            iteration_data = {\n",
    "                'version': version,\n",
    "                'iteration': iteration,\n",
    "                'prompt': current_prompt,\n",
    "                'evaluation': eval_result,\n",
    "                'score': eval_result['total_score'],\n",
    "                'percentage': eval_result['percentage'],\n",
    "                'below_threshold_count': eval_result['below_threshold_count']\n",
    "            }\n",
    "            self.history.append(iteration_data)\n",
    "            \n",
    "            # Update best result if this is better\n",
    "            if self.best_result is None or eval_result['total_score'] > self.best_result['score']:\n",
    "                self.best_result = {\n",
    "                    'version': version,\n",
    "                    'iteration': iteration,\n",
    "                    'prompt': current_prompt,\n",
    "                    'evaluation': eval_result,\n",
    "                    'score': eval_result['total_score'],\n",
    "                    'percentage': eval_result['percentage']\n",
    "                }\n",
    "                print(f\"\\nüèÜ NEW BEST SCORE: {eval_result['total_score']:.2f}/150 ({eval_result['percentage']:.1f}%)\")\n",
    "            \n",
    "            # Check stopping conditions\n",
    "            if eval_result['total_score'] >= target_score:\n",
    "                print(f\"\\n‚úÖ TARGET SCORE REACHED: {eval_result['total_score']:.2f} >= {target_score}\")\n",
    "                print(f\"Stopping after {iteration} iterations.\")\n",
    "                break\n",
    "            \n",
    "            if eval_result['below_threshold_count'] == 0:\n",
    "                print(f\"\\n‚úÖ ALL METRICS ABOVE THRESHOLD!\")\n",
    "                print(f\"Stopping after {iteration} iterations.\")\n",
    "                break\n",
    "            \n",
    "            # Check if we should continue\n",
    "            if iteration >= max_iterations:\n",
    "                print(f\"\\n‚èπÔ∏è  MAX ITERATIONS REACHED ({max_iterations})\")\n",
    "                break\n",
    "            \n",
    "            # Check for minimal improvement\n",
    "            if iteration > 1:\n",
    "                previous_score = self.history[-2]['score']\n",
    "                score_change = eval_result['total_score'] - previous_score\n",
    "                \n",
    "                if score_change < min_improvement:\n",
    "                    print(f\"\\n‚èπÔ∏è  MINIMAL IMPROVEMENT: +{score_change:.2f} points < {min_improvement} threshold\")\n",
    "                    print(f\"Stopping after {iteration} iterations.\")\n",
    "                    break\n",
    "            \n",
    "            # Generate improved prompt for next iteration\n",
    "            print(f\"\\n{'‚îÄ'*80}\")\n",
    "            print(f\"Generating improved prompt for next iteration...\")\n",
    "            print(f\"{'‚îÄ'*80}\")\n",
    "            \n",
    "            improvement_result = self.improver.improve_prompt(current_prompt, eval_result)\n",
    "            \n",
    "            # Store improvement data\n",
    "            iteration_data['improvement'] = improvement_result\n",
    "            \n",
    "            print(f\"\\nüìù Changes planned:\")\n",
    "            changes_preview = improvement_result['change_explanations'][:300]\n",
    "            print(changes_preview + \"...\" if len(improvement_result['change_explanations']) > 300 else changes_preview)\n",
    "            \n",
    "            # Update current prompt for next iteration\n",
    "            current_prompt = improvement_result['improved_prompt']\n",
    "        \n",
    "        # Optimization complete\n",
    "        return self._compile_optimization_results(iteration)\n",
    "    \n",
    "    def _compile_optimization_results(self, total_iterations: int) -> Dict:\n",
    "        \"\"\"Compile final optimization results.\"\"\"\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"OPTIMIZATION COMPLETE\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Calculate improvement statistics\n",
    "        first_score = self.history[0]['score']\n",
    "        final_score = self.history[-1]['score']\n",
    "        best_score = self.best_result['score']\n",
    "        \n",
    "        total_improvement = best_score - first_score\n",
    "        improvement_pct = (total_improvement / first_score * 100) if first_score > 0 else 0\n",
    "        \n",
    "        print(f\"\\nüìä Summary:\")\n",
    "        print(f\"   Total Iterations: {total_iterations}\")\n",
    "        print(f\"   First Score: {first_score:.2f}/150 ({self.history[0]['percentage']:.1f}%)\")\n",
    "        print(f\"   Final Score: {final_score:.2f}/150 ({self.history[-1]['percentage']:.1f}%)\")\n",
    "        print(f\"   Best Score: {best_score:.2f}/150 ({self.best_result['percentage']:.1f}%) - {self.best_result['version']}\")\n",
    "        print(f\"   Total Improvement: +{total_improvement:.2f} points (+{improvement_pct:.1f}%)\")\n",
    "        \n",
    "        if self.best_result['version'] != f\"v{total_iterations}\":\n",
    "            print(f\"\\n‚ö†Ô∏è  Note: Best prompt is from {self.best_result['version']}, not final iteration\")\n",
    "        \n",
    "        # Compile results\n",
    "        results = {\n",
    "            'total_iterations': total_iterations,\n",
    "            'history': self.history,\n",
    "            'best_result': self.best_result,\n",
    "            'first_score': first_score,\n",
    "            'final_score': final_score,\n",
    "            'best_score': best_score,\n",
    "            'total_improvement': total_improvement,\n",
    "            'improvement_percentage': improvement_pct,\n",
    "            'target_reached': best_score >= self.config['target_score']\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_best_prompt(self) -> Optional[str]:\n",
    "        \"\"\"Get the best prompt from all iterations.\"\"\"\n",
    "        if self.best_result:\n",
    "            return self.best_result['prompt']\n",
    "        return None\n",
    "    \n",
    "    def get_improvement_trajectory(self) -> List[Dict]:\n",
    "        \"\"\"Get score trajectory across iterations.\"\"\"\n",
    "        return [\n",
    "            {\n",
    "                'iteration': h['iteration'],\n",
    "                'version': h['version'],\n",
    "                'score': h['score'],\n",
    "                'percentage': h['percentage'],\n",
    "                'below_threshold': h['below_threshold_count']\n",
    "            }\n",
    "            for h in self.history\n",
    "        ]\n",
    "    \n",
    "    def generate_optimization_report(self, results: Dict) -> str:\n",
    "        \"\"\"Generate comprehensive optimization report.\"\"\"\n",
    "        \n",
    "        lines = []\n",
    "        lines.append(\"=\"*100)\n",
    "        lines.append(\"PROMPT OPTIMIZATION REPORT\")\n",
    "        lines.append(\"=\"*100)\n",
    "        lines.append(\"\")\n",
    "        \n",
    "        # Summary\n",
    "        lines.append(\"SUMMARY\")\n",
    "        lines.append(\"-\"*100)\n",
    "        lines.append(f\"Total Iterations: {results['total_iterations']}\")\n",
    "        lines.append(f\"Starting Score: {results['first_score']:.2f}/150 ({self.history[0]['percentage']:.1f}%)\")\n",
    "        lines.append(f\"Final Score: {results['final_score']:.2f}/150 ({self.history[-1]['percentage']:.1f}%)\")\n",
    "        lines.append(f\"Best Score: {results['best_score']:.2f}/150 ({results['best_result']['percentage']:.1f}%) \"\n",
    "                    f\"[{results['best_result']['version']}]\")\n",
    "        lines.append(f\"Total Improvement: +{results['total_improvement']:.2f} points \"\n",
    "                    f\"(+{results['improvement_percentage']:.1f}%)\")\n",
    "        lines.append(f\"Target Reached: {'‚úÖ Yes' if results['target_reached'] else '‚ùå No'}\")\n",
    "        lines.append(\"\")\n",
    "        \n",
    "        # Iteration-by-iteration breakdown\n",
    "        lines.append(\"ITERATION HISTORY\")\n",
    "        lines.append(\"-\"*100)\n",
    "        lines.append(f\"{'Version':<10} {'Score':<15} {'Change':<15} {'Below Threshold':<20} {'Status':<20}\")\n",
    "        lines.append(\"-\"*100)\n",
    "        \n",
    "        for i, iteration in enumerate(self.history):\n",
    "            version = iteration['version']\n",
    "            score = iteration['score']\n",
    "            below = iteration['below_threshold_count']\n",
    "            \n",
    "            if i == 0:\n",
    "                change_str = \"Initial\"\n",
    "            else:\n",
    "                change = score - self.history[i-1]['score']\n",
    "                change_str = f\"+{change:.2f}\" if change >= 0 else f\"{change:.2f}\"\n",
    "            \n",
    "            is_best = (version == results['best_result']['version'])\n",
    "            status = \"üèÜ BEST\" if is_best else \"\"\n",
    "            \n",
    "            lines.append(f\"{version:<10} {score:.2f}/150{'':<5} {change_str:<15} {below} metrics{'':<10} {status:<20}\")\n",
    "        \n",
    "        lines.append(\"\")\n",
    "        \n",
    "        # Best prompt details\n",
    "        lines.append(\"=\"*100)\n",
    "        lines.append(f\"BEST PROMPT ({results['best_result']['version']})\")\n",
    "        lines.append(\"=\"*100)\n",
    "        lines.append(\"\")\n",
    "        lines.append(results['best_result']['prompt'])\n",
    "        lines.append(\"\")\n",
    "        \n",
    "        # Best prompt metrics breakdown\n",
    "        lines.append(\"=\"*100)\n",
    "        lines.append(\"BEST PROMPT EVALUATION BREAKDOWN\")\n",
    "        lines.append(\"=\"*100)\n",
    "        \n",
    "        best_eval = results['best_result']['evaluation']\n",
    "        for group_name, group_data in best_eval['group_scores'].items():\n",
    "            status = \"‚úÖ\" if group_data['percentage'] >= 70 else \"‚ö†Ô∏è\"\n",
    "            lines.append(f\"\\n{status} {group_name.upper().replace('_', ' ')}: \"\n",
    "                        f\"{group_data['score']:.2f}/{group_data['max']} ({group_data['percentage']:.1f}%)\")\n",
    "        \n",
    "        lines.append(\"\")\n",
    "        \n",
    "        # Improvements made across all iterations\n",
    "        lines.append(\"=\"*100)\n",
    "        lines.append(\"IMPROVEMENTS MADE ACROSS ITERATIONS\")\n",
    "        lines.append(\"=\"*100)\n",
    "        \n",
    "        for i, iteration in enumerate(self.history):\n",
    "            if 'improvement' in iteration:\n",
    "                lines.append(f\"\\n{iteration['version']} ‚Üí v{i+2}:\")\n",
    "                lines.append(\"-\"*100)\n",
    "                lines.append(iteration['improvement']['change_explanations'])\n",
    "        \n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "print(\"‚úÖ Prompt Optimizer (Iteration Manager) Created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f25814",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_optimization_pipeline(api_key: str = None, use_mock: bool = True):\n",
    "    \"\"\"Test the complete optimization pipeline.\"\"\"\n",
    "    \n",
    "    # Test with different complexity levels\n",
    "    test_cases = {\n",
    "        \"simple\": \"Write a blog post about AI\",\n",
    "        \n",
    "        \"moderate\": \"\"\"\n",
    "Write a technical article about machine learning.\n",
    "Include examples and keep it under 2000 words.\n",
    "\"\"\",\n",
    "        \n",
    "        \"structured\": \"\"\"\n",
    "# Task: Technical Guide\n",
    "\n",
    "Create a guide comparing transformer architectures.\n",
    "\n",
    "Requirements:\n",
    "1. Explain BERT, GPT, and T5\n",
    "2. Include code examples\n",
    "3. Target audience: ML engineers\n",
    "\n",
    "Keep it technically accurate.\n",
    "\"\"\"\n",
    "    }\n",
    "    \n",
    "    # Choose test case\n",
    "    test_name = \"moderate\"\n",
    "    test_prompt = test_cases[test_name]\n",
    "    \n",
    "    print(f\"Testing Optimization Pipeline: '{test_name}' prompt\")\n",
    "    print(f\"Using {'MOCK' if use_mock else 'REAL'} evaluation/improvement\")\n",
    "    print()\n",
    "    \n",
    "    # Initialize optimizer\n",
    "    if use_mock:\n",
    "        optimizer = PromptOptimizer(METRIC_CONFIG, CONFIG, api_key=None)\n",
    "        \n",
    "        # Replace with mock versions\n",
    "        def mock_eval(prompt, first_run=True, below_threshold_metrics=None):\n",
    "            # Call custom evaluator (real)\n",
    "            custom_results = optimizer.evaluator.custom_evaluator.evaluate_all(prompt)\n",
    "            \n",
    "            # Mock LLM evaluation\n",
    "            llm_metrics = {\n",
    "                name: config for name, config in METRIC_CONFIG.items()\n",
    "                if config['type'] == 'llm_judge'\n",
    "            }\n",
    "            if not first_run and below_threshold_metrics:\n",
    "                llm_metrics = {\n",
    "                    name: config for name, config in llm_metrics.items()\n",
    "                    if name in below_threshold_metrics\n",
    "                }\n",
    "            \n",
    "            llm_results = mock_llm_evaluation(prompt, list(llm_metrics.keys()))\n",
    "            \n",
    "            all_results = {**custom_results, **llm_results}\n",
    "            return optimizer.evaluator._compile_results(prompt, all_results, first_run)\n",
    "        \n",
    "        optimizer.evaluator.evaluate = mock_eval\n",
    "        \n",
    "        def mock_improve(original, eval_result):\n",
    "            return _mock_improve_prompt(original, eval_result)\n",
    "        \n",
    "        optimizer.improver.improve_prompt = mock_improve\n",
    "    else:\n",
    "        if not api_key:\n",
    "            print(\"‚ö†Ô∏è  API key required for real optimization\")\n",
    "            return None\n",
    "        optimizer = PromptOptimizer(METRIC_CONFIG, CONFIG, api_key=api_key)\n",
    "    \n",
    "    # Run optimization\n",
    "    start_time = time.time()\n",
    "    results = optimizer.optimize(test_prompt)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n‚è±Ô∏è  Total Time: {elapsed_time:.2f} seconds\")\n",
    "    \n",
    "    # Generate report\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"GENERATING OPTIMIZATION REPORT\")\n",
    "    print(\"=\"*100)\n",
    "    report = optimizer.generate_optimization_report(results)\n",
    "    \n",
    "    # Show report preview\n",
    "    report_lines = report.split('\\n')\n",
    "    print('\\n'.join(report_lines[:80]))  # Show first 80 lines\n",
    "    if len(report_lines) > 80:\n",
    "        print(f\"\\n... ({len(report_lines) - 80} more lines in full report) ...\")\n",
    "    \n",
    "    return optimizer, results, report\n",
    "\n",
    "\n",
    "# Run test\n",
    "print(\"=\"*100)\n",
    "print(\"RUNNING FULL OPTIMIZATION PIPELINE TEST\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "optimizer, opt_results, opt_report = test_optimization_pipeline(use_mock=True)\n",
    "\n",
    "if opt_results:\n",
    "    print(\"\\n‚úÖ Optimization complete!\")\n",
    "    print(f\"   Optimizer stored in 'optimizer' variable\")\n",
    "    print(f\"   Results stored in 'opt_results' variable\")\n",
    "    print(f\"   Report stored in 'opt_report' variable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1bb0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_optimization_progress(results: Dict):\n",
    "    \"\"\"Visualize the optimization progress across iterations.\"\"\"\n",
    "    \n",
    "    history = results['history']\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Prompt Optimization Progress', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Score Progression\n",
    "    ax1 = axes[0, 0]\n",
    "    iterations = [h['iteration'] for h in history]\n",
    "    scores = [h['score'] for h in history]\n",
    "    \n",
    "    ax1.plot(iterations, scores, marker='o', linewidth=2, markersize=8, color='steelblue')\n",
    "    ax1.axhline(y=CONFIG['target_score'], color='green', linestyle='--', \n",
    "                label=f'Target ({CONFIG[\"target_score\"]})', linewidth=2)\n",
    "    \n",
    "    # Highlight best\n",
    "    best_iter = results['best_result']['iteration']\n",
    "    best_score = results['best_result']['score']\n",
    "    ax1.scatter([best_iter], [best_score], color='gold', s=300, marker='*', \n",
    "                zorder=5, label='Best', edgecolors='black', linewidth=2)\n",
    "    \n",
    "    ax1.set_xlabel('Iteration', fontsize=12)\n",
    "    ax1.set_ylabel('Total Score (out of 150)', fontsize=12)\n",
    "    ax1.set_title('Score Progression', fontsize=14, fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.legend()\n",
    "    ax1.set_xticks(iterations)\n",
    "    \n",
    "    # 2. Below-Threshold Metrics Count\n",
    "    ax2 = axes[0, 1]\n",
    "    below_counts = [h['below_threshold_count'] for h in history]\n",
    "    \n",
    "    colors = ['red' if c > 0 else 'green' for c in below_counts]\n",
    "    ax2.bar(iterations, below_counts, color=colors, alpha=0.7, edgecolor='black')\n",
    "    ax2.set_xlabel('Iteration', fontsize=12)\n",
    "    ax2.set_ylabel('Metrics Below Threshold', fontsize=12)\n",
    "    ax2.set_title('Metrics Needing Improvement', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    ax2.set_xticks(iterations)\n",
    "    \n",
    "    # 3. Score Change Per Iteration\n",
    "    ax3 = axes[1, 0]\n",
    "    score_changes = [0]  # First iteration has no change\n",
    "    for i in range(1, len(history)):\n",
    "        change = history[i]['score'] - history[i-1]['score']\n",
    "        score_changes.append(change)\n",
    "    \n",
    "    colors_change = ['green' if c >= 0 else 'red' for c in score_changes]\n",
    "    bars = ax3.bar(iterations, score_changes, color=colors_change, alpha=0.7, edgecolor='black')\n",
    "    ax3.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "    ax3.set_xlabel('Iteration', fontsize=12)\n",
    "    ax3.set_ylabel('Score Change', fontsize=12)\n",
    "    ax3.set_title('Score Improvement Per Iteration', fontsize=14, fontweight='bold')\n",
    "    ax3.grid(True, alpha=0.3, axis='y')\n",
    "    ax3.set_xticks(iterations)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, change in zip(bars, score_changes):\n",
    "        if change != 0:\n",
    "            height = bar.get_height()\n",
    "            label_y = height + (0.5 if height > 0 else -1.5)\n",
    "            ax3.text(bar.get_x() + bar.get_width()/2., label_y,\n",
    "                    f'{change:+.1f}', ha='center', va='bottom' if height > 0 else 'top',\n",
    "                    fontsize=9, fontweight='bold')\n",
    "    \n",
    "    # 4. Group Scores Heatmap (for best version)\n",
    "    ax4 = axes[1, 1]\n",
    "    best_eval = results['best_result']['evaluation']\n",
    "    groups = list(best_eval['group_scores'].keys())\n",
    "    group_percentages = [best_eval['group_scores'][g]['percentage'] for g in groups]\n",
    "    \n",
    "    # Create color map based on percentage\n",
    "    colors_heat = []\n",
    "    for pct in group_percentages:\n",
    "        if pct >= 80:\n",
    "            colors_heat.append('darkgreen')\n",
    "        elif pct >= 70:\n",
    "            colors_heat.append('green')\n",
    "        elif pct >= 50:\n",
    "            colors_heat.append('orange')\n",
    "        else:\n",
    "            colors_heat.append('red')\n",
    "    \n",
    "    y_pos = range(len(groups))\n",
    "    bars = ax4.barh(y_pos, group_percentages, color=colors_heat, alpha=0.7, edgecolor='black')\n",
    "    ax4.axvline(x=70, color='red', linestyle='--', label='Threshold (70%)', linewidth=2)\n",
    "    ax4.set_yticks(y_pos)\n",
    "    ax4.set_yticklabels([g.replace('_', ' ').title() for g in groups], fontsize=10)\n",
    "    ax4.set_xlabel('Percentage', fontsize=12)\n",
    "    ax4.set_title(f'Best Version ({results[\"best_result\"][\"version\"]}) - Group Scores', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3, axis='x')\n",
    "    ax4.set_xlim(0, 100)\n",
    "    \n",
    "    # Add percentage labels\n",
    "    for bar, pct in zip(bars, group_percentages):\n",
    "        width = bar.get_width()\n",
    "        ax4.text(width + 2, bar.get_y() + bar.get_height()/2.,\n",
    "                f'{pct:.1f}%', ha='left', va='center', fontsize=9, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "# Visualize optimization results\n",
    "if 'opt_results' in globals():\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"GENERATING OPTIMIZATION VISUALIZATION\")\n",
    "    print(\"=\"*100)\n",
    "    fig = visualize_optimization_progress(opt_results)\n",
    "    plt.show()\n",
    "    print(\"‚úÖ Visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83078931",
   "metadata": {},
   "source": [
    "# Step 7: Results, Visualization & Demo Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594bcff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import pandas as pd\n",
    "\n",
    "class PromptOptimizationDemo:\n",
    "    \"\"\"\n",
    "    Complete demo package for prompt optimization system.\n",
    "    Provides easy-to-use interface with visualization and export capabilities.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str = None, use_mock: bool = True):\n",
    "        \"\"\"\n",
    "        Initialize the demo system.\n",
    "        \n",
    "        Args:\n",
    "            api_key: Anthropic API key (optional, required for real evaluation)\n",
    "            use_mock: If True, use mock evaluation for demo purposes\n",
    "        \"\"\"\n",
    "        self.use_mock = use_mock\n",
    "        self.api_key = api_key\n",
    "        \n",
    "        # Initialize optimizer\n",
    "        if use_mock:\n",
    "            print(\"üîß Initializing in MOCK mode (no API calls)\")\n",
    "            self.optimizer = PromptOptimizer(METRIC_CONFIG, CONFIG, api_key=None)\n",
    "            self._setup_mock_mode()\n",
    "        else:\n",
    "            if not api_key:\n",
    "                raise ValueError(\"API key required when use_mock=False\")\n",
    "            print(\"üîß Initializing in REAL mode (will make API calls)\")\n",
    "            self.optimizer = PromptOptimizer(METRIC_CONFIG, CONFIG, api_key=api_key)\n",
    "        \n",
    "        print(\"‚úÖ Demo system ready!\")\n",
    "    \n",
    "    def _setup_mock_mode(self):\n",
    "        \"\"\"Setup mock evaluation and improvement methods.\"\"\"\n",
    "        \n",
    "        # Mock evaluator\n",
    "        def mock_eval(prompt, first_run=True, below_threshold_metrics=None):\n",
    "            custom_results = self.optimizer.evaluator.custom_evaluator.evaluate_all(prompt)\n",
    "            \n",
    "            llm_metrics = {\n",
    "                name: config for name, config in METRIC_CONFIG.items()\n",
    "                if config['type'] == 'llm_judge'\n",
    "            }\n",
    "            if not first_run and below_threshold_metrics:\n",
    "                llm_metrics = {\n",
    "                    name: config for name, config in llm_metrics.items()\n",
    "                    if name in below_threshold_metrics\n",
    "                }\n",
    "            \n",
    "            llm_results = mock_llm_evaluation(prompt, list(llm_metrics.keys()))\n",
    "            all_results = {**custom_results, **llm_results}\n",
    "            return self.optimizer.evaluator._compile_results(prompt, all_results, first_run)\n",
    "        \n",
    "        self.optimizer.evaluator.evaluate = mock_eval\n",
    "        \n",
    "        # Mock improver\n",
    "        def mock_improve(original, eval_result):\n",
    "            return _mock_improve_prompt(original, eval_result)\n",
    "        \n",
    "        self.optimizer.improver.improve_prompt = mock_improve\n",
    "    \n",
    "    def run_optimization(self, prompt: str, max_iterations: int = None,\n",
    "                        target_score: float = None, verbose: bool = True) -> Dict:\n",
    "        \"\"\"\n",
    "        Run complete optimization on a prompt.\n",
    "        \n",
    "        Args:\n",
    "            prompt: Initial prompt to optimize\n",
    "            max_iterations: Max iterations (default: 5)\n",
    "            target_score: Target score (default: 120)\n",
    "            verbose: Print progress (default: True)\n",
    "        \n",
    "        Returns:\n",
    "            Complete optimization results\n",
    "        \"\"\"\n",
    "        if not verbose:\n",
    "            # Suppress print statements\n",
    "            import sys\n",
    "            import io\n",
    "            old_stdout = sys.stdout\n",
    "            sys.stdout = io.StringIO()\n",
    "        \n",
    "        try:\n",
    "            results = self.optimizer.optimize(\n",
    "                prompt,\n",
    "                max_iterations=max_iterations,\n",
    "                target_score=target_score\n",
    "            )\n",
    "        finally:\n",
    "            if not verbose:\n",
    "                sys.stdout = old_stdout\n",
    "        \n",
    "        if verbose:\n",
    "            self.print_summary(results)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def print_summary(self, results: Dict):\n",
    "        \"\"\"Print optimization summary.\"\"\"\n",
    "        print(\"\\n\" + \"üéØ \"*40)\n",
    "        print(\"OPTIMIZATION COMPLETE - SUMMARY\")\n",
    "        print(\"üéØ \"*40)\n",
    "        print(f\"\\nüìä Results:\")\n",
    "        print(f\"   ‚Ä¢ Starting Score: {results['first_score']:.2f}/150 ({results['first_score']/150*100:.1f}%)\")\n",
    "        print(f\"   ‚Ä¢ Best Score: {results['best_score']:.2f}/150 ({results['best_result']['percentage']:.1f}%)\")\n",
    "        print(f\"   ‚Ä¢ Final Score: {results['final_score']:.2f}/150 ({results['final_score']/150*100:.1f}%)\")\n",
    "        print(f\"   ‚Ä¢ Improvement: +{results['total_improvement']:.2f} points (+{results['improvement_percentage']:.1f}%)\")\n",
    "        print(f\"   ‚Ä¢ Best Version: {results['best_result']['version']} (iteration {results['best_result']['iteration']})\")\n",
    "        print(f\"   ‚Ä¢ Total Iterations: {results['total_iterations']}\")\n",
    "        print(f\"\\n{'‚úÖ TARGET REACHED!' if results['target_reached'] else '‚ö†Ô∏è  Target not reached'}\")\n",
    "    \n",
    "    def get_best_prompt(self) -> str:\n",
    "        \"\"\"Get the best optimized prompt.\"\"\"\n",
    "        return self.optimizer.get_best_prompt()\n",
    "    \n",
    "    def visualize_all(self, results: Dict, save_path: str = None):\n",
    "        \"\"\"Create comprehensive visualization dashboard.\"\"\"\n",
    "        fig = self._create_dashboard(results)\n",
    "        \n",
    "        if save_path:\n",
    "            fig.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"üìä Visualization saved to: {save_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "        return fig\n",
    "    \n",
    "    def _create_dashboard(self, results: Dict) -> plt.Figure:\n",
    "        \"\"\"Create comprehensive dashboard.\"\"\"\n",
    "        fig = plt.figure(figsize=(20, 14))\n",
    "        gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "        \n",
    "        # Title\n",
    "        fig.suptitle(f'Prompt Optimization Dashboard - Best: {results[\"best_score\"]:.1f}/150 '\n",
    "                    f'({results[\"best_result\"][\"percentage\"]:.1f}%)',\n",
    "                    fontsize=18, fontweight='bold')\n",
    "        \n",
    "        history = results['history']\n",
    "        \n",
    "        # 1. Score Timeline (large, top)\n",
    "        ax1 = fig.add_subplot(gs[0, :])\n",
    "        iterations = [h['iteration'] for h in history]\n",
    "        scores = [h['score'] for h in history]\n",
    "        \n",
    "        ax1.plot(iterations, scores, marker='o', linewidth=3, markersize=10, \n",
    "                color='steelblue', label='Score')\n",
    "        ax1.axhline(y=CONFIG['target_score'], color='green', linestyle='--', \n",
    "                   linewidth=2, label=f'Target ({CONFIG[\"target_score\"]})')\n",
    "        \n",
    "        # Highlight best\n",
    "        best_iter = results['best_result']['iteration']\n",
    "        best_score = results['best_result']['score']\n",
    "        ax1.scatter([best_iter], [best_score], color='gold', s=500, marker='*', \n",
    "                   zorder=5, edgecolors='black', linewidth=2, label='Best')\n",
    "        \n",
    "        # Fill area\n",
    "        ax1.fill_between(iterations, scores, alpha=0.3, color='steelblue')\n",
    "        \n",
    "        ax1.set_xlabel('Iteration', fontsize=12, fontweight='bold')\n",
    "        ax1.set_ylabel('Total Score (out of 150)', fontsize=12, fontweight='bold')\n",
    "        ax1.set_title('Score Progression Over Iterations', fontsize=14, fontweight='bold')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax1.legend(fontsize=11)\n",
    "        ax1.set_xticks(iterations)\n",
    "        \n",
    "        # 2. Metrics Below Threshold\n",
    "        ax2 = fig.add_subplot(gs[1, 0])\n",
    "        below_counts = [h['below_threshold_count'] for h in history]\n",
    "        colors = ['#d32f2f' if c > 10 else '#ff9800' if c > 5 else '#4caf50' for c in below_counts]\n",
    "        bars = ax2.bar(iterations, below_counts, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "        ax2.set_xlabel('Iteration', fontsize=11, fontweight='bold')\n",
    "        ax2.set_ylabel('Count', fontsize=11, fontweight='bold')\n",
    "        ax2.set_title('Metrics Below Threshold', fontsize=12, fontweight='bold')\n",
    "        ax2.grid(True, alpha=0.3, axis='y')\n",
    "        ax2.set_xticks(iterations)\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, count in zip(bars, below_counts):\n",
    "            height = bar.get_height()\n",
    "            if height > 0:\n",
    "                ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                        f'{int(count)}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # 3. Score Changes\n",
    "        ax3 = fig.add_subplot(gs[1, 1])\n",
    "        score_changes = [0]\n",
    "        for i in range(1, len(history)):\n",
    "            change = history[i]['score'] - history[i-1]['score']\n",
    "            score_changes.append(change)\n",
    "        \n",
    "        colors_change = ['#4caf50' if c > 0 else '#d32f2f' if c < 0 else '#757575' \n",
    "                        for c in score_changes]\n",
    "        bars = ax3.bar(iterations, score_changes, color=colors_change, alpha=0.8, \n",
    "                      edgecolor='black', linewidth=1.5)\n",
    "        ax3.axhline(y=0, color='black', linestyle='-', linewidth=1)\n",
    "        ax3.set_xlabel('Iteration', fontsize=11, fontweight='bold')\n",
    "        ax3.set_ylabel('Score Change', fontsize=11, fontweight='bold')\n",
    "        ax3.set_title('Score Change Per Iteration', fontsize=12, fontweight='bold')\n",
    "        ax3.grid(True, alpha=0.3, axis='y')\n",
    "        ax3.set_xticks(iterations)\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, change in zip(bars, score_changes):\n",
    "            if change != 0:\n",
    "                height = bar.get_height()\n",
    "                label_y = height + (0.3 if height > 0 else -0.8)\n",
    "                ax3.text(bar.get_x() + bar.get_width()/2., label_y,\n",
    "                        f'{change:+.1f}', ha='center', \n",
    "                        va='bottom' if height > 0 else 'top',\n",
    "                        fontweight='bold', fontsize=9)\n",
    "        \n",
    "        # 4. Overall Score Gauge\n",
    "        ax4 = fig.add_subplot(gs[1, 2])\n",
    "        self._draw_gauge(ax4, results['best_score'], 150, \n",
    "                        title='Best Score', subtitle=results['best_result']['version'])\n",
    "        \n",
    "        # 5. Group Scores Heatmap\n",
    "        ax5 = fig.add_subplot(gs[2, :2])\n",
    "        best_eval = results['best_result']['evaluation']\n",
    "        groups = list(best_eval['group_scores'].keys())\n",
    "        group_percentages = [best_eval['group_scores'][g]['percentage'] for g in groups]\n",
    "        \n",
    "        colors_heat = []\n",
    "        for pct in group_percentages:\n",
    "            if pct >= 80:\n",
    "                colors_heat.append('#1b5e20')\n",
    "            elif pct >= 70:\n",
    "                colors_heat.append('#4caf50')\n",
    "            elif pct >= 50:\n",
    "                colors_heat.append('#ff9800')\n",
    "            else:\n",
    "                colors_heat.append('#d32f2f')\n",
    "        \n",
    "        y_pos = range(len(groups))\n",
    "        bars = ax5.barh(y_pos, group_percentages, color=colors_heat, alpha=0.8, \n",
    "                       edgecolor='black', linewidth=1.5)\n",
    "        ax5.axvline(x=70, color='red', linestyle='--', linewidth=2, label='Threshold (70%)')\n",
    "        ax5.set_yticks(y_pos)\n",
    "        ax5.set_yticklabels([g.replace('_', ' ').title() for g in groups], fontsize=11)\n",
    "        ax5.set_xlabel('Percentage', fontsize=11, fontweight='bold')\n",
    "        ax5.set_title(f'Group Scores - Best Version ({results[\"best_result\"][\"version\"]})', \n",
    "                     fontsize=12, fontweight='bold')\n",
    "        ax5.legend(fontsize=10)\n",
    "        ax5.grid(True, alpha=0.3, axis='x')\n",
    "        ax5.set_xlim(0, 100)\n",
    "        \n",
    "        # Add percentage labels\n",
    "        for bar, pct in zip(bars, group_percentages):\n",
    "            width = bar.get_width()\n",
    "            ax5.text(width + 2, bar.get_y() + bar.get_height()/2.,\n",
    "                    f'{pct:.1f}%', ha='left', va='center', fontweight='bold', fontsize=10)\n",
    "        \n",
    "        # 6. Improvement Statistics\n",
    "        ax6 = fig.add_subplot(gs[2, 2])\n",
    "        ax6.axis('off')\n",
    "        \n",
    "        stats_text = f\"\"\"\n",
    "        üìà IMPROVEMENT STATS\n",
    "        \n",
    "        Starting: {results['first_score']:.1f}/150\n",
    "        ({results['first_score']/150*100:.1f}%)\n",
    "        \n",
    "        Best: {results['best_score']:.1f}/150\n",
    "        ({results['best_result']['percentage']:.1f}%)\n",
    "        \n",
    "        Improvement: +{results['total_improvement']:.1f}\n",
    "        (+{results['improvement_percentage']:.1f}%)\n",
    "        \n",
    "        Iterations: {results['total_iterations']}\n",
    "        \n",
    "        Best at: {results['best_result']['version']}\n",
    "        \n",
    "        Status: {'‚úÖ Target Reached' if results['target_reached'] else '‚ö†Ô∏è In Progress'}\n",
    "        \"\"\"\n",
    "        \n",
    "        ax6.text(0.1, 0.5, stats_text, fontsize=11, verticalalignment='center',\n",
    "                family='monospace', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "        \n",
    "        return fig\n",
    "    \n",
    "    def _draw_gauge(self, ax, value, max_value, title=\"\", subtitle=\"\"):\n",
    "        \"\"\"Draw a gauge chart.\"\"\"\n",
    "        ax.set_xlim(0, 10)\n",
    "        ax.set_ylim(0, 10)\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Background circle\n",
    "        circle_bg = plt.Circle((5, 5), 3, color='lightgray', fill=True, alpha=0.3)\n",
    "        ax.add_patch(circle_bg)\n",
    "        \n",
    "        # Color based on percentage\n",
    "        percentage = (value / max_value) * 100\n",
    "        if percentage >= 80:\n",
    "            color = '#4caf50'\n",
    "        elif percentage >= 60:\n",
    "            color = '#ff9800'\n",
    "        else:\n",
    "            color = '#d32f2f'\n",
    "        \n",
    "        # Value circle\n",
    "        circle_val = plt.Circle((5, 5), 2.5, color=color, fill=True, alpha=0.8)\n",
    "        ax.add_patch(circle_val)\n",
    "        \n",
    "        # Text\n",
    "        ax.text(5, 5.5, f\"{value:.1f}\", ha='center', va='center', \n",
    "               fontsize=24, fontweight='bold', color='white')\n",
    "        ax.text(5, 4.2, f\"/ {max_value}\", ha='center', va='center', \n",
    "               fontsize=14, color='white')\n",
    "        \n",
    "        if title:\n",
    "            ax.text(5, 8.5, title, ha='center', va='center', \n",
    "                   fontsize=12, fontweight='bold')\n",
    "        if subtitle:\n",
    "            ax.text(5, 1.5, subtitle, ha='center', va='center', \n",
    "                   fontsize=10, style='italic')\n",
    "    \n",
    "    def export_results(self, results: Dict, output_dir: str = \"./optimization_results\"):\n",
    "        \"\"\"Export all results to files.\"\"\"\n",
    "        \n",
    "        # Create output directory\n",
    "        output_path = Path(output_dir)\n",
    "        output_path.mkdir(exist_ok=True)\n",
    "        \n",
    "        print(f\"\\nüìÅ Exporting results to: {output_path.absolute()}\")\n",
    "        \n",
    "        # 1. Save best prompt\n",
    "        best_prompt_file = output_path / \"best_prompt.txt\"\n",
    "        with open(best_prompt_file, 'w') as f:\n",
    "            f.write(results['best_result']['prompt'])\n",
    "        print(f\"   ‚úÖ Best prompt saved: {best_prompt_file.name}\")\n",
    "        \n",
    "        # 2. Save full report\n",
    "        report = self.optimizer.generate_optimization_report(results)\n",
    "        report_file = output_path / \"optimization_report.txt\"\n",
    "        with open(report_file, 'w') as f:\n",
    "            f.write(report)\n",
    "        print(f\"   ‚úÖ Full report saved: {report_file.name}\")\n",
    "        \n",
    "        # 3. Save iteration history as CSV\n",
    "        history_data = []\n",
    "        for h in results['history']:\n",
    "            history_data.append({\n",
    "                'iteration': h['iteration'],\n",
    "                'version': h['version'],\n",
    "                'score': h['score'],\n",
    "                'percentage': h['percentage'],\n",
    "                'below_threshold_count': h['below_threshold_count']\n",
    "            })\n",
    "        \n",
    "        df = pd.DataFrame(history_data)\n",
    "        csv_file = output_path / \"iteration_history.csv\"\n",
    "        df.to_csv(csv_file, index=False)\n",
    "        print(f\"   ‚úÖ History saved: {csv_file.name}\")\n",
    "        \n",
    "        # 4. Save all prompts\n",
    "        prompts_dir = output_path / \"prompts\"\n",
    "        prompts_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        for h in results['history']:\n",
    "            prompt_file = prompts_dir / f\"{h['version']}_prompt.txt\"\n",
    "            with open(prompt_file, 'w') as f:\n",
    "                f.write(h['prompt'])\n",
    "        print(f\"   ‚úÖ All prompts saved: {prompts_dir.name}/\")\n",
    "        \n",
    "        # 5. Save visualization\n",
    "        fig = self._create_dashboard(results)\n",
    "        viz_file = output_path / \"optimization_dashboard.png\"\n",
    "        fig.savefig(viz_file, dpi=300, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "        print(f\"   ‚úÖ Dashboard saved: {viz_file.name}\")\n",
    "        \n",
    "        # 6. Save JSON results\n",
    "        import json\n",
    "        \n",
    "        # Create JSON-serializable version\n",
    "        json_results = {\n",
    "            'total_iterations': results['total_iterations'],\n",
    "            'first_score': results['first_score'],\n",
    "            'final_score': results['final_score'],\n",
    "            'best_score': results['best_score'],\n",
    "            'total_improvement': results['total_improvement'],\n",
    "            'improvement_percentage': results['improvement_percentage'],\n",
    "            'target_reached': results['target_reached'],\n",
    "            'best_version': results['best_result']['version'],\n",
    "            'best_iteration': results['best_result']['iteration'],\n",
    "            'history': history_data\n",
    "        }\n",
    "        \n",
    "        json_file = output_path / \"results.json\"\n",
    "        with open(json_file, 'w') as f:\n",
    "            json.dump(json_results, f, indent=2)\n",
    "        print(f\"   ‚úÖ JSON results saved: {json_file.name}\")\n",
    "        \n",
    "        print(f\"\\n‚úÖ All results exported successfully!\")\n",
    "        print(f\"üìÇ Location: {output_path.absolute()}\")\n",
    "        \n",
    "        return output_path\n",
    "\n",
    "\n",
    "print(\"‚úÖ Demo Package Created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7623f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_complete_demo():\n",
    "    \"\"\"\n",
    "    Run a complete demonstration of the prompt optimization system.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*100)\n",
    "    print(\" \"*30 + \"üöÄ PROMPT OPTIMIZATION SYSTEM DEMO üöÄ\")\n",
    "    print(\"=\"*100)\n",
    "    print()\n",
    "    print(\"This demo will:\")\n",
    "    print(\"  1. Initialize the optimization system\")\n",
    "    print(\"  2. Evaluate an initial prompt (scoring 0-150)\")\n",
    "    print(\"  3. Iteratively improve the prompt\")\n",
    "    print(\"  4. Track and return the best version\")\n",
    "    print(\"  5. Generate comprehensive reports and visualizations\")\n",
    "    print()\n",
    "    print(\"=\"*100)\n",
    "    input(\"Press ENTER to begin demo...\")\n",
    "    \n",
    "    # Demo prompts\n",
    "    demo_prompts = {\n",
    "        \"minimal\": \"Explain quantum computing.\",\n",
    "        \n",
    "        \"basic\": \"\"\"\n",
    "Write an article about artificial intelligence.\n",
    "Include recent developments and future trends.\n",
    "Target audience: general readers.\n",
    "\"\"\",\n",
    "        \n",
    "        \"moderate\": \"\"\"\n",
    "Create a technical guide about transformer models in NLP.\n",
    "\n",
    "Requirements:\n",
    "- Explain the architecture\n",
    "- Compare BERT vs GPT\n",
    "- Include practical use cases\n",
    "- Keep it under 2000 words\n",
    "\"\"\"\n",
    "    }\n",
    "    \n",
    "    # Let user choose\n",
    "    print(\"\\nüìù Available test prompts:\")\n",
    "    for i, (name, prompt) in enumerate(demo_prompts.items(), 1):\n",
    "        preview = prompt[:80].replace('\\n', ' ')\n",
    "        print(f\"  {i}. {name}: {preview}...\")\n",
    "    \n",
    "    choice = input(\"\\nSelect prompt (1-3) or press ENTER for 'basic': \").strip()\n",
    "    \n",
    "    if choice == \"1\":\n",
    "        prompt_name = \"minimal\"\n",
    "    elif choice == \"3\":\n",
    "        prompt_name = \"moderate\"\n",
    "    else:\n",
    "        prompt_name = \"basic\"\n",
    "    \n",
    "    test_prompt = demo_prompts[prompt_name]\n",
    "    \n",
    "    print(f\"\\n‚úÖ Using '{prompt_name}' prompt\")\n",
    "    print(f\"\\nInitial Prompt:\")\n",
    "    print(\"‚îÄ\" * 100)\n",
    "    print(test_prompt)\n",
    "    print(\"‚îÄ\" * 100)\n",
    "    \n",
    "    input(\"\\nPress ENTER to start optimization...\")\n",
    "    \n",
    "    # Initialize demo system\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    demo = PromptOptimizationDemo(use_mock=True)\n",
    "    \n",
    "    # Run optimization\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"STARTING OPTIMIZATION\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    results = demo.run_optimization(\n",
    "        test_prompt,\n",
    "        max_iterations=5,\n",
    "        target_score=120,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Show best prompt\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"BEST OPTIMIZED PROMPT\")\n",
    "    print(\"=\"*100)\n",
    "    best_prompt = demo.get_best_prompt()\n",
    "    print(best_prompt)\n",
    "    print()\n",
    "    \n",
    "    # Visualize\n",
    "    input(\"\\nPress ENTER to see visualization...\")\n",
    "    demo.visualize_all(results)\n",
    "    \n",
    "    # Export option\n",
    "    export = input(\"\\nüíæ Export results to files? (y/n): \").strip().lower()\n",
    "    if export == 'y':\n",
    "        output_dir = input(\"Enter output directory (default: ./optimization_results): \").strip()\n",
    "        if not output_dir:\n",
    "            output_dir = \"./optimization_results\"\n",
    "        demo.export_results(results, output_dir)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"‚úÖ DEMO COMPLETE!\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"\\nüìä Summary:\")\n",
    "    print(f\"   ‚Ä¢ Iterations: {results['total_iterations']}\")\n",
    "    print(f\"   ‚Ä¢ Improvement: {results['first_score']:.1f} ‚Üí {results['best_score']:.1f} \"\n",
    "          f\"(+{results['total_improvement']:.1f} points)\")\n",
    "    print(f\"   ‚Ä¢ Best Version: {results['best_result']['version']}\")\n",
    "    print(f\"   ‚Ä¢ Target Reached: {'Yes ‚úÖ' if results['target_reached'] else 'No ‚ö†Ô∏è'}\")\n",
    "    \n",
    "    return demo, results\n",
    "\n",
    "\n",
    "# Optional: Quick start functions for different use cases\n",
    "def quick_evaluate(prompt: str, use_mock: bool = True, api_key: str = None):\n",
    "    \"\"\"Quick evaluation of a single prompt.\"\"\"\n",
    "    demo = PromptOptimizationDemo(api_key=api_key, use_mock=use_mock)\n",
    "    evaluator = demo.optimizer.evaluator\n",
    "    result = evaluator.evaluate(prompt, first_run=True)\n",
    "    report = evaluator.generate_detailed_report(result)\n",
    "    print(report)\n",
    "    return result\n",
    "\n",
    "\n",
    "def quick_optimize(prompt: str, use_mock: bool = True, api_key: str = None):\n",
    "    \"\"\"Quick optimization with default settings.\"\"\"\n",
    "    demo = PromptOptimizationDemo(api_key=api_key, use_mock=use_mock)\n",
    "    results = demo.run_optimization(prompt)\n",
    "    demo.visualize_all(results)\n",
    "    return demo, results\n",
    "\n",
    "\n",
    "print(\"‚úÖ Demo Script Ready!\")\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"üé¨ READY TO RUN DEMO\")\n",
    "print(\"=\"*100)\n",
    "print(\"\\nAvailable functions:\")\n",
    "print(\"  ‚Ä¢ run_complete_demo() - Interactive full demo\")\n",
    "print(\"  ‚Ä¢ quick_evaluate(prompt) - Just evaluate a prompt\")\n",
    "print(\"  ‚Ä¢ quick_optimize(prompt) - Optimize with defaults\")\n",
    "print(\"\\nOr use the PromptOptimizationDemo class directly:\")\n",
    "print(\"  demo = PromptOptimizationDemo(use_mock=True)\")\n",
    "print(\"  results = demo.run_optimization(your_prompt)\")\n",
    "print(\"  demo.visualize_all(results)\")\n",
    "print(\"  demo.export_results(results)\")\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65af88bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a quick reference card\n",
    "def print_quick_reference():\n",
    "    \"\"\"Print quick reference guide.\"\"\"\n",
    "    \n",
    "    reference = \"\"\"\n",
    "    ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "    ‚ïë                     PROMPT OPTIMIZATION SYSTEM - QUICK REFERENCE              ‚ïë\n",
    "    ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "    \n",
    "    üìä SCORING SYSTEM\n",
    "    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    Total Score: 0-150 points\n",
    "    \n",
    "    Group Distribution:\n",
    "      ‚Ä¢ Structure & Clarity        25 pts (5 metrics √ó 5 pts)\n",
    "      ‚Ä¢ Context & Information      25 pts (5 metrics √ó 5 pts)\n",
    "      ‚Ä¢ Reasoning & Cognition      30 pts (6 metrics √ó 5 pts)  ‚Üê Highest priority\n",
    "      ‚Ä¢ Safety & Alignment         30 pts (6 metrics √ó 5 pts)  ‚Üê Highest priority\n",
    "      ‚Ä¢ Format & Style             15 pts (5 metrics √ó 3 pts)\n",
    "      ‚Ä¢ Output Quality             20 pts (6 metrics √ó 3.33 pts)\n",
    "      ‚Ä¢ Advanced Features           5 pts (3 metrics √ó 1.67 pts)\n",
    "    \n",
    "    Threshold: 70% of max points per metric\n",
    "    Target: 120/150 (80%)\n",
    "    \n",
    "    \n",
    "    üîß BASIC USAGE\n",
    "    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    \n",
    "    # Initialize\n",
    "    demo = PromptOptimizationDemo(use_mock=True)  # or use_mock=False with api_key\n",
    "    \n",
    "    # Optimize\n",
    "    results = demo.run_optimization(\n",
    "        prompt=\"Your prompt here\",\n",
    "        max_iterations=5,\n",
    "        target_score=120\n",
    "    )\n",
    "    \n",
    "    # Get best prompt\n",
    "    best_prompt = demo.get_best_prompt()\n",
    "    \n",
    "    # Visualize\n",
    "    demo.visualize_all(results)\n",
    "    \n",
    "    # Export\n",
    "    demo.export_results(results, \"./my_results\")\n",
    "    \n",
    "    \n",
    "    üìà OPTIMIZATION FLOW\n",
    "    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    \n",
    "    1. Initial Evaluation (all 37 metrics)\n",
    "       ‚îú‚îÄ Custom Functions (10 metrics) - pattern matching\n",
    "       ‚îî‚îÄ LLM Judge (27 metrics) - Claude API\n",
    "    \n",
    "    2. Generate Improvement\n",
    "       ‚îî‚îÄ Target ALL below-threshold metrics\n",
    "    \n",
    "    3. Re-Evaluate (only below-threshold metrics)\n",
    "    \n",
    "    4. Track Best (across all iterations)\n",
    "    \n",
    "    5. Repeat until:\n",
    "       ‚îú‚îÄ Target score reached (120/150)\n",
    "       ‚îú‚îÄ Max iterations (5)\n",
    "       ‚îî‚îÄ Minimal improvement (<3 pts)\n",
    "    \n",
    "    \n",
    "    üéØ KEY FEATURES\n",
    "    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    \n",
    "    ‚úÖ Comprehensive Evaluation: 37 metrics across 7 categories\n",
    "    ‚úÖ Iterative Improvement: Automatic prompt enhancement\n",
    "    ‚úÖ Best Tracking: Always returns highest-scoring version\n",
    "    ‚úÖ Efficient Re-evaluation: Only re-checks improved metrics\n",
    "    ‚úÖ Mock Mode: Test without API calls\n",
    "    ‚úÖ Visualization: Interactive charts and dashboards\n",
    "    ‚úÖ Export: Reports, prompts, and data in multiple formats\n",
    "    \n",
    "    \n",
    "    üí° TIPS\n",
    "    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    \n",
    "    ‚Ä¢ Start with mock mode to understand the system\n",
    "    ‚Ä¢ Use real API for production optimization\n",
    "    ‚Ä¢ Review improvement explanations to learn patterns\n",
    "    ‚Ä¢ Best prompt may not be the final iteration (system tracks this!)\n",
    "    ‚Ä¢ Export results to track progress over time\n",
    "    \n",
    "    \n",
    "    üìû NEED HELP?\n",
    "    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    \n",
    "    run_complete_demo()      - Interactive guided demo\n",
    "    quick_evaluate(prompt)   - Just evaluate, don't optimize\n",
    "    quick_optimize(prompt)   - Quick optimization with defaults\n",
    "    print_quick_reference()  - Show this guide again\n",
    "    \n",
    "    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "    \"\"\"\n",
    "    \n",
    "    print(reference)\n",
    "\n",
    "# Print on load\n",
    "print_quick_reference()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9482e54f",
   "metadata": {},
   "source": [
    "# FINAL SUMMARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef0f4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_final_summary_document():\n",
    "    \"\"\"Generate comprehensive final summary document.\"\"\"\n",
    "    \n",
    "    summary = \"\"\"\n",
    "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "‚ïë                                                                               ‚ïë\n",
    "‚ïë              PROMPT EVALUATION & OPTIMIZATION SYSTEM                          ‚ïë\n",
    "‚ïë                         FINAL SUMMARY DOCUMENT                                ‚ïë\n",
    "‚ïë                                                                               ‚ïë\n",
    "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "\n",
    "\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "üìã EXECUTIVE SUMMARY\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "SYSTEM PURPOSE:\n",
    "A scientific, data-driven system for evaluating and optimizing prompts through\n",
    "iterative refinement. The system measures prompts across 37 quality dimensions,\n",
    "identifies weaknesses, generates improvements, and tracks the best version across\n",
    "all iterations.\n",
    "\n",
    "KEY CAPABILITIES:\n",
    "‚úÖ Comprehensive evaluation on 0-150 point scale\n",
    "‚úÖ 37 metrics across 7 quality dimensions\n",
    "‚úÖ Automated iterative improvement\n",
    "‚úÖ Best-prompt tracking (prevents regression)\n",
    "‚úÖ Dual evaluation approach (pattern matching + LLM judge)\n",
    "‚úÖ Visualization and reporting\n",
    "‚úÖ Mock mode for testing without API costs\n",
    "\n",
    "\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "üèóÔ∏è SYSTEM ARCHITECTURE\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "COMPONENTS:\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ 1. METRIC CONFIGURATION                                                     ‚îÇ\n",
    "‚îÇ    ‚Ä¢ 37 metrics organized into 7 groups                                     ‚îÇ\n",
    "‚îÇ    ‚Ä¢ Group-based point allocation (150 total)                               ‚îÇ\n",
    "‚îÇ    ‚Ä¢ 70% threshold per metric                                               ‚îÇ\n",
    "‚îÇ    ‚Ä¢ Prioritization: Reasoning & Safety (30 pts each)                       ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ 2. CUSTOM FUNCTION EVALUATOR                                                ‚îÇ\n",
    "‚îÇ    ‚Ä¢ 10 metrics via pattern matching                                        ‚îÇ\n",
    "‚îÇ    ‚Ä¢ Rule-based detection (regex, keyword analysis)                         ‚îÇ\n",
    "‚îÇ    ‚Ä¢ Instant evaluation, no API calls                                       ‚îÇ\n",
    "‚îÇ    ‚Ä¢ Metrics: Structure, Examples, Roles, Validation, etc.                  ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ 3. LLM JUDGE EVALUATOR                                                      ‚îÇ\n",
    "‚îÇ    ‚Ä¢ 27 metrics via Claude API                                              ‚îÇ\n",
    "‚îÇ    ‚Ä¢ Subjective quality assessment                                          ‚îÇ\n",
    "‚îÇ    ‚Ä¢ Batched evaluation (5 metrics per call)                                ‚îÇ\n",
    "‚îÇ    ‚Ä¢ Metrics: Clarity, Reasoning, Safety, Alignment, etc.                   ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ 4. FULL EVALUATOR PIPELINE                                                  ‚îÇ\n",
    "‚îÇ    ‚Ä¢ Combines both evaluator types                                          ‚îÇ\n",
    "‚îÇ    ‚Ä¢ Calculates total score (0-150)                                         ‚îÇ\n",
    "‚îÇ    ‚Ä¢ Identifies below-threshold metrics                                     ‚îÇ\n",
    "‚îÇ    ‚Ä¢ Group-level analysis                                                   ‚îÇ\n",
    "‚îÇ    ‚Ä¢ Generates detailed reports                                             ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ 5. PROMPT IMPROVER                                                          ‚îÇ\n",
    "‚îÇ    ‚Ä¢ Targets ALL below-threshold metrics                                    ‚îÇ\n",
    "‚îÇ    ‚Ä¢ Uses Claude API for intelligent improvements                           ‚îÇ\n",
    "‚îÇ    ‚Ä¢ Preserves original intent                                              ‚îÇ\n",
    "‚îÇ    ‚Ä¢ Returns: improved prompt + change explanations                         ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ 6. ITERATION MANAGER (PromptOptimizer)                                      ‚îÇ\n",
    "‚îÇ    ‚Ä¢ Orchestrates evaluation ‚Üí improvement ‚Üí re-evaluation                  ‚îÇ\n",
    "‚îÇ    ‚Ä¢ Tracks best prompt across ALL iterations                               ‚îÇ\n",
    "‚îÇ    ‚Ä¢ Efficient re-evaluation (only below-threshold metrics)                 ‚îÇ\n",
    "‚îÇ    ‚Ä¢ Multiple stopping criteria                                             ‚îÇ\n",
    "‚îÇ    ‚Ä¢ Comprehensive history tracking                                         ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ 7. DEMO PACKAGE (PromptOptimizationDemo)                                    ‚îÇ\n",
    "‚îÇ    ‚Ä¢ User-friendly interface                                                ‚îÇ\n",
    "‚îÇ    ‚Ä¢ Mock mode for testing                                                  ‚îÇ\n",
    "‚îÇ    ‚Ä¢ Visualization dashboard                                                ‚îÇ\n",
    "‚îÇ    ‚Ä¢ Multi-format export                                                    ‚îÇ\n",
    "‚îÇ    ‚Ä¢ Quick-start functions                                                  ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "üìä SCORING SYSTEM DETAILS\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "TOTAL: 150 POINTS\n",
    "\n",
    "GROUP BREAKDOWN:\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ Group                      ‚îÇ Points ‚îÇ Metrics  ‚îÇ Pts/Metric ‚îÇ Priority     ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ Structure & Clarity        ‚îÇ   25   ‚îÇ    5     ‚îÇ    5.0     ‚îÇ Medium       ‚îÇ\n",
    "‚îÇ Context & Information      ‚îÇ   25   ‚îÇ    5     ‚îÇ    5.0     ‚îÇ Medium       ‚îÇ\n",
    "‚îÇ Reasoning & Cognition      ‚îÇ   30   ‚îÇ    6     ‚îÇ    5.0     ‚îÇ ‚≠ê HIGHEST   ‚îÇ\n",
    "‚îÇ Safety & Alignment         ‚îÇ   30   ‚îÇ    6     ‚îÇ    5.0     ‚îÇ ‚≠ê HIGHEST   ‚îÇ\n",
    "‚îÇ Format & Style             ‚îÇ   15   ‚îÇ    5     ‚îÇ    3.0     ‚îÇ Low          ‚îÇ\n",
    "‚îÇ Output Quality             ‚îÇ   20   ‚îÇ    6     ‚îÇ    3.33    ‚îÇ Medium       ‚îÇ\n",
    "‚îÇ Advanced Features          ‚îÇ    5   ‚îÇ    3     ‚îÇ    1.67    ‚îÇ Low          ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "EVALUATION TYPES:\n",
    "- Custom Functions: 10 metrics (instant, pattern-based)\n",
    "- LLM Judge: 27 metrics (API-based, subjective assessment)\n",
    "\n",
    "THRESHOLDS:\n",
    "- Per-Metric Threshold: 70% of maximum points\n",
    "- Target Score: 120/150 (80%)\n",
    "- Minimum Improvement: 3 points to continue iterating\n",
    "\n",
    "\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "üîÑ OPTIMIZATION WORKFLOW\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "ITERATION 1 (First Run):\n",
    "‚îú‚îÄ Evaluate ALL 37 metrics\n",
    "‚îÇ  ‚îú‚îÄ Custom functions: 10 metrics\n",
    "‚îÇ  ‚îî‚îÄ LLM judge: 27 metrics (in batches of 5)\n",
    "‚îú‚îÄ Calculate total score\n",
    "‚îú‚îÄ Identify below-threshold metrics\n",
    "‚îî‚îÄ Generate improved prompt (targeting ALL below-threshold)\n",
    "\n",
    "ITERATION 2-N (Subsequent Runs):\n",
    "‚îú‚îÄ Evaluate ONLY below-threshold metrics from previous iteration\n",
    "‚îÇ  ‚îî‚îÄ Efficiency: Skip metrics already above threshold\n",
    "‚îú‚îÄ Calculate total score\n",
    "‚îú‚îÄ Update best prompt if score improved\n",
    "‚îú‚îÄ Check stopping conditions:\n",
    "‚îÇ  ‚îú‚îÄ Target score reached (120/150)\n",
    "‚îÇ  ‚îú‚îÄ All metrics above threshold\n",
    "‚îÇ  ‚îú‚îÄ Max iterations reached (5)\n",
    "‚îÇ  ‚îî‚îÄ Minimal improvement (<3 points)\n",
    "‚îî‚îÄ If continuing: Generate next improvement\n",
    "\n",
    "FINAL OUTPUT:\n",
    "‚îî‚îÄ Return BEST prompt from ANY iteration (not just final)\n",
    "   ‚îú‚îÄ Prevents regression\n",
    "   ‚îî‚îÄ Scientific approach: track highest quality achieved\n",
    "\n",
    "\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "üéØ KEY DESIGN DECISIONS & RATIONALE\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "1. GROUP-BASED SCORING (Not Equal Weight)\n",
    "   Rationale: Prioritizes critical areas like reasoning and safety\n",
    "   Impact: Guides improvement toward most important quality dimensions\n",
    "\n",
    "2. DUAL EVALUATION APPROACH\n",
    "   Rationale: Combines speed (custom) with depth (LLM judge)\n",
    "   Impact: Faster, cheaper, yet comprehensive evaluation\n",
    "\n",
    "3. BATCHED LLM EVALUATION\n",
    "   Rationale: Reduces API calls while maintaining quality\n",
    "   Impact: 27 metrics evaluated in ~6 API calls instead of 27\n",
    "\n",
    "4. RE-EVALUATE ONLY BELOW-THRESHOLD\n",
    "   Rationale: Efficiency - don't re-check what's already good\n",
    "   Impact: Subsequent iterations much faster than first run\n",
    "\n",
    "5. BEST PROMPT TRACKING\n",
    "   Rationale: Iterative improvement can sometimes regress\n",
    "   Impact: Always return the highest-quality version achieved\n",
    "\n",
    "6. 70% THRESHOLD PER METRIC\n",
    "   Rationale: Balance between achievability and quality\n",
    "   Impact: Prompts must be \"good\" not just \"present\" on each dimension\n",
    "\n",
    "7. TARGET ALL BELOW-THRESHOLD SIMULTANEOUSLY\n",
    "   Rationale: Holistic improvement vs. incremental fixing\n",
    "   Impact: Faster convergence to high-quality prompts\n",
    "\n",
    "\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "üìà EXPECTED PERFORMANCE\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "TYPICAL OPTIMIZATION TRAJECTORY:\n",
    "\n",
    "Initial Prompt (Minimal):\n",
    "‚îî‚îÄ Score: 40-60/150 (27-40%)\n",
    "   Many structural and reasoning elements missing\n",
    "\n",
    "After 1 Iteration:\n",
    "‚îî‚îÄ Score: 70-90/150 (47-60%)\n",
    "   Basic structure, examples, and clarity added\n",
    "\n",
    "After 2-3 Iterations:\n",
    "‚îî‚îÄ Score: 95-115/150 (63-77%)\n",
    "   Refinement of reasoning, safety, and completeness\n",
    "\n",
    "After 4-5 Iterations:\n",
    "‚îî‚îÄ Score: 110-130/150 (73-87%)\n",
    "   Polished, comprehensive prompts\n",
    "\n",
    "API USAGE (Real Mode):\n",
    "- First iteration: ~8-10 API calls\n",
    "- Subsequent iterations: ~2-5 API calls (depending on metrics improved)\n",
    "- Total for 5 iterations: ~20-30 API calls\n",
    "\n",
    "TIME ESTIMATE:\n",
    "- Mock mode: ~30 seconds for 5 iterations\n",
    "- Real mode: ~2-4 minutes for 5 iterations (API latency)\n",
    "\n",
    "\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "üõ†Ô∏è USAGE EXAMPLES\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "BASIC USAGE:\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "from prompt_optimizer import PromptOptimizationDemo\n",
    "\n",
    "# Initialize (mock mode - no API calls)\n",
    "demo = PromptOptimizationDemo(use_mock=True)\n",
    "\n",
    "# Optimize a prompt\n",
    "results = demo.run_optimization(\n",
    "    prompt=\"Write a blog post about AI\",\n",
    "    max_iterations=5,\n",
    "    target_score=120\n",
    ")\n",
    "\n",
    "# Get best prompt\n",
    "best = demo.get_best_prompt()\n",
    "print(best)\n",
    "\n",
    "# Visualize\n",
    "demo.visualize_all(results)\n",
    "\n",
    "# Export everything\n",
    "demo.export_results(results, \"./my_results\")\n",
    "\n",
    "\n",
    "PRODUCTION USAGE (With API):\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Initialize with API key\n",
    "demo = PromptOptimizationDemo(\n",
    "    api_key=\"sk-ant-...\",\n",
    "    use_mock=False\n",
    ")\n",
    "\n",
    "# Optimize\n",
    "results = demo.run_optimization(\n",
    "    prompt=your_prompt,\n",
    "    max_iterations=3,\n",
    "    target_score=120\n",
    ")\n",
    "\n",
    "\n",
    "QUICK EVALUATION (No Optimization):\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "result = quick_evaluate(\"Your prompt here\", use_mock=True)\n",
    "# Returns evaluation report\n",
    "\n",
    "\n",
    "INTERACTIVE DEMO:\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "run_complete_demo()\n",
    "# Guided walkthrough with visualization\n",
    "\n",
    "\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "üì¶ OUTPUT FORMATS\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "EXPORTED FILES:\n",
    "‚îú‚îÄ‚îÄ best_prompt.txt              (The highest-scoring prompt)\n",
    "‚îú‚îÄ‚îÄ optimization_report.txt      (Complete iteration history & analysis)\n",
    "‚îú‚îÄ‚îÄ iteration_history.csv        (Tabular data for external analysis)\n",
    "‚îú‚îÄ‚îÄ results.json                 (Machine-readable results)\n",
    "‚îú‚îÄ‚îÄ optimization_dashboard.png   (Visualization)\n",
    "‚îî‚îÄ‚îÄ prompts/\n",
    "    ‚îú‚îÄ‚îÄ v1_prompt.txt\n",
    "    ‚îú‚îÄ‚îÄ v2_prompt.txt\n",
    "    ‚îú‚îÄ‚îÄ v3_prompt.txt\n",
    "    ‚îî‚îÄ‚îÄ ...\n",
    "\n",
    "VISUALIZATION DASHBOARD INCLUDES:\n",
    "- Score progression timeline with best marker\n",
    "- Below-threshold metrics count per iteration\n",
    "- Score change per iteration (improvement/regression)\n",
    "- Overall score gauge\n",
    "- Group performance heatmap\n",
    "- Detailed statistics panel\n",
    "\n",
    "\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "‚úÖ SUCCESS CRITERIA (MET)\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "‚úì Scientific Process\n",
    "  ‚Üí Measurable metrics, data-driven improvements, documented methodology\n",
    "\n",
    "‚úì 0-150 Scoring Scale\n",
    "  ‚Üí Group-based point allocation with 37 metrics\n",
    "\n",
    "‚úì Iterative Optimization\n",
    "  ‚Üí Automatic improvement loop with multiple stopping criteria\n",
    "\n",
    "‚úì Best Prompt Tracking\n",
    "  ‚Üí System always returns highest-scoring version\n",
    "\n",
    "‚úì Efficiency\n",
    "  ‚Üí First run: all metrics; Subsequent: only below-threshold\n",
    "\n",
    "‚úì Demo-Ready\n",
    "  ‚Üí Complete demo package with visualization and export\n",
    "\n",
    "‚úì Extensible\n",
    "  ‚Üí Easy to add new metrics or modify scoring\n",
    "\n",
    "‚úì Mock Mode\n",
    "  ‚Üí Test without API costs\n",
    "\n",
    "\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "üîÆ FUTURE ENHANCEMENTS (Optional)\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "Possible Extensions:\n",
    "- Multi-prompt comparison mode\n",
    "- Custom metric weights by use case\n",
    "- A/B testing framework\n",
    "- Integration with prompt libraries\n",
    "- Real-time evaluation API endpoint\n",
    "- Batch processing for multiple prompts\n",
    "- Historical trend analysis\n",
    "- Domain-specific metric sets\n",
    "\n",
    "\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "üìû SUPPORT & REFERENCE\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "QUICK COMMANDS:\n",
    "- run_complete_demo()           - Full interactive demo\n",
    "- quick_evaluate(prompt)        - Evaluate only\n",
    "- quick_optimize(prompt)        - Optimize with defaults\n",
    "- print_quick_reference()       - Show quick reference\n",
    "\n",
    "DOCUMENTATION:\n",
    "- All code is documented with docstrings\n",
    "- Inline comments explain complex logic\n",
    "- Example usage throughout\n",
    "\n",
    "KEY CLASSES:\n",
    "- PromptOptimizationDemo        - Main interface\n",
    "- PromptOptimizer               - Core orchestrator\n",
    "- PromptEvaluator               - Unified evaluation\n",
    "- PromptImprover                - Improvement generation\n",
    "- CustomMetricEvaluator         - Pattern-based metrics\n",
    "- LLMJudgeEvaluator            - LLM-based metrics\n",
    "\n",
    "\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "üéâ CONCLUSION\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "This system provides a complete, scientific approach to prompt optimization:\n",
    "\n",
    "‚úÖ Comprehensive evaluation across 37 quality dimensions\n",
    "‚úÖ Intelligent iterative improvement\n",
    "‚úÖ Best-version tracking (no regression)\n",
    "‚úÖ Production-ready with mock testing capability\n",
    "‚úÖ Extensive visualization and reporting\n",
    "‚úÖ Easy to use, easy to extend\n",
    "\n",
    "The system successfully transforms vague prompts into high-quality, well-\n",
    "structured prompts through measurable, data-driven iteration.\n",
    "\n",
    "READY FOR DEMONSTRATION AND DEPLOYMENT! üöÄ\n",
    "\n",
    "\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\"\"\"\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Generate and display summary\n",
    "summary_doc = generate_final_summary_document()\n",
    "print(summary_doc)\n",
    "\n",
    "# Save summary to file\n",
    "with open(\"SYSTEM_SUMMARY.txt\", \"w\") as f:\n",
    "    f.write(summary_doc)\n",
    "\n",
    "print(\"\\n‚úÖ Summary document saved to: SYSTEM_SUMMARY.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e724e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\n\" + \"=\"*100)\n",
    "print(\"=\"*100)\n",
    "print(\" \"*35 + \"üß™ RUNNING TEST OPTIMIZATION üß™\")\n",
    "print(\"=\"*100)\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Initialize demo system\n",
    "print(\"\\nüì¶ Initializing system...\")\n",
    "demo = PromptOptimizationDemo(use_mock=True)\n",
    "\n",
    "# Test prompt - intentionally basic to show improvement\n",
    "test_prompt = \"\"\"\n",
    "Write a technical article about machine learning for beginners.\n",
    "Include some examples and keep it interesting.\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"INITIAL PROMPT\")\n",
    "print(\"=\"*100)\n",
    "print(test_prompt)\n",
    "print()\n",
    "\n",
    "# Run optimization\n",
    "print(\"=\"*100)\n",
    "print(\"STARTING OPTIMIZATION (This will take ~30 seconds in mock mode)\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "results = demo.run_optimization(\n",
    "    prompt=test_prompt,\n",
    "    max_iterations=5,\n",
    "    target_score=120,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "# Show results\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"üèÜ BEST OPTIMIZED PROMPT üèÜ\")\n",
    "print(\"=\"*100)\n",
    "best_prompt = demo.get_best_prompt()\n",
    "print(best_prompt)\n",
    "print()\n",
    "\n",
    "# Print iteration trajectory\n",
    "print(\"=\"*100)\n",
    "print(\"üìà OPTIMIZATION TRAJECTORY\")\n",
    "print(\"=\"*100)\n",
    "trajectory = demo.optimizer.get_improvement_trajectory()\n",
    "print(f\"\\n{'Iteration':<12} {'Version':<10} {'Score':<15} {'Percentage':<12} {'Below Threshold':<20}\")\n",
    "print(\"-\" * 100)\n",
    "for t in trajectory:\n",
    "    print(f\"{t['iteration']:<12} {t['version']:<10} {t['score']:<15.2f} {t['percentage']:<12.1f}% {t['below_threshold']:<20}\")\n",
    "\n",
    "# Show key improvements\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"üîç KEY IMPROVEMENTS MADE\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "for i, iteration in enumerate(results['history']):\n",
    "    if 'improvement' in iteration:\n",
    "        print(f\"\\n{iteration['version']} ‚Üí v{i+2}:\")\n",
    "        print(\"-\" * 100)\n",
    "        changes = iteration['improvement']['change_explanations']\n",
    "        # Show first 500 chars of changes\n",
    "        print(changes[:500] + \"...\" if len(changes) > 500 else changes)\n",
    "\n",
    "# Statistics\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"üìä FINAL STATISTICS\")\n",
    "print(\"=\"*100)\n",
    "print(f\"\"\"\n",
    "Total Time: {elapsed:.2f} seconds\n",
    "Total Iterations: {results['total_iterations']}\n",
    "\n",
    "SCORES:\n",
    "  Starting: {results['first_score']:.2f}/150 ({results['first_score']/150*100:.1f}%)\n",
    "  Best:     {results['best_score']:.2f}/150 ({results['best_result']['percentage']:.1f}%)\n",
    "  Final:    {results['final_score']:.2f}/150 ({results['final_score']/150*100:.1f}%)\n",
    "\n",
    "IMPROVEMENT:\n",
    "  Total Gain: +{results['total_improvement']:.2f} points\n",
    "  Percentage: +{results['improvement_percentage']:.1f}%\n",
    "  \n",
    "BEST VERSION:\n",
    "  Version: {results['best_result']['version']}\n",
    "  Iteration: {results['best_result']['iteration']}\n",
    "  \n",
    "TARGET:\n",
    "  Goal: 120/150 (80%)\n",
    "  Achieved: {'‚úÖ YES' if results['target_reached'] else '‚ùå NO'}\n",
    "\n",
    "GROUP SCORES (Best Version):\n",
    "\"\"\")\n",
    "\n",
    "for group, data in results['best_result']['evaluation']['group_scores'].items():\n",
    "    status = \"‚úÖ\" if data['percentage'] >= 70 else \"‚ö†Ô∏è\"\n",
    "    print(f\"  {status} {group.replace('_', ' ').title():<30} {data['score']:>6.2f}/{data['max']:<6.0f} ({data['percentage']:>5.1f}%)\")\n",
    "\n",
    "# Visualize\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"üìä GENERATING VISUALIZATION\")\n",
    "print(\"=\"*100)\n",
    "demo.visualize_all(results)\n",
    "\n",
    "# Offer to export\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"üíæ EXPORT OPTIONS\")\n",
    "print(\"=\"*100)\n",
    "print(\"Results can be exported with: demo.export_results(results, './my_results')\")\n",
    "print(\"This will save:\")\n",
    "print(\"  ‚Ä¢ Best prompt (TXT)\")\n",
    "print(\"  ‚Ä¢ Full report (TXT)\")\n",
    "print(\"  ‚Ä¢ Iteration history (CSV)\")\n",
    "print(\"  ‚Ä¢ All prompts (TXT)\")\n",
    "print(\"  ‚Ä¢ Visualization (PNG)\")\n",
    "print(\"  ‚Ä¢ Results data (JSON)\")\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"=\"*100)\n",
    "print(\" \"*40 + \"‚úÖ TEST COMPLETE! ‚úÖ\")\n",
    "print(\"=\"*100)\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(f\"\"\"\n",
    "üéâ SYSTEM DEMONSTRATION SUCCESSFUL!\n",
    "\n",
    "The prompt went from {results['first_score']:.1f} points to {results['best_score']:.1f} points \n",
    "through {results['total_iterations']} iterations of scientific optimization.\n",
    "\n",
    "Key Achievements:\n",
    "‚úì Comprehensive evaluation (37 metrics)\n",
    "‚úì Iterative improvement\n",
    "‚úì Best version tracking\n",
    "‚úì Detailed reporting\n",
    "‚úì Visual progress tracking\n",
    "\n",
    "The system is ready for production use! üöÄ\n",
    "\"\"\")\n",
    "\n",
    "# Store results for further exploration\n",
    "test_results = results\n",
    "test_demo = demo\n",
    "\n",
    "print(\"\\nüìå Results stored in variables:\")\n",
    "print(\"   ‚Ä¢ test_results - Full optimization results\")\n",
    "print(\"   ‚Ä¢ test_demo - Demo instance (use for export)\")\n",
    "print(\"\\nExample: test_demo.export_results(test_results, './my_optimization')\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
